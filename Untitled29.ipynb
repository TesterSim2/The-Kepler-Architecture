{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNNZHvkzoKuT00IoKHYh5B2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TesterSim2/The-Kepler-Architecture/blob/main/Untitled29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from transformers import LlamaConfig, LlamaForCausalLM\n",
        "\n",
        "# ==========================================\n",
        "# 1. OPTIMIZER IMPLEMENTATIONS\n",
        "# ==========================================\n",
        "\n",
        "class FAdam(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Implements FAdam (Fisher Adam) from Hwang (2024).\n",
        "    Key differences from AdamW:\n",
        "    1. Preconditioning (1/sqrt(v)) happens BEFORE Momentum.\n",
        "    2. Adaptive Epsilon based on gradient RMS.\n",
        "    3. Natural Gradient logic.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0.0, adam_w_mode=True):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
        "                        adam_w_mode=adam_w_mode)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "\n",
        "            beta1, beta2 = group['betas']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    params_with_grad.append(p)\n",
        "                    if p.grad.is_sparse:\n",
        "                        raise RuntimeError('FAdam does not support sparse gradients')\n",
        "                    grads.append(p.grad)\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    # Lazy state initialization\n",
        "                    if len(state) == 0:\n",
        "                        state['step'] = 0\n",
        "                        # FIM momentum (init to 1 per paper suggestions or 0, paper says 1 for metric)\n",
        "                        # We stick to standard 0 init for stability in FP16 usually,\n",
        "                        # but paper Alg 1 says f_0 <- 1. Let's try 0 to avoid massive initial steps.\n",
        "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    exp_avgs.append(state['exp_avg'])\n",
        "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "                    state['step'] += 1\n",
        "                    state_steps.append(state['step'])\n",
        "\n",
        "            for i, param in enumerate(params_with_grad):\n",
        "                grad = grads[i]\n",
        "                exp_avg = exp_avgs[i]\n",
        "                exp_avg_sq = exp_avg_sqs[i]\n",
        "                step = state_steps[i]\n",
        "\n",
        "                # 1. Update Empirical Fisher (v_t)\n",
        "                # v_t = beta2 * v_{t-1} + (1-beta2) * g^2\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # 2. Bias Correction for v_t\n",
        "                bias_correction2 = 1 - beta2 ** step\n",
        "                v_hat = exp_avg_sq / bias_correction2\n",
        "\n",
        "                # 3. Adaptive Epsilon (Appendix B.3)\n",
        "                # eps_hat = min(eps, 0.01 * RMS(g))\n",
        "                grad_rms = torch.sqrt(torch.mean(grad ** 2))\n",
        "                adaptive_eps = min(group['eps'], 0.01 * grad_rms.item())\n",
        "\n",
        "                # 4. Calculate Natural Gradient (Pre-Momentum Scaling)\n",
        "                # g_nat = g / (sqrt(v_hat) + eps)\n",
        "                denom = v_hat.sqrt().add_(adaptive_eps)\n",
        "                natural_grad = grad / denom\n",
        "\n",
        "                # 5. Update Momentum on the Natural Gradient (Riemannian Momentum)\n",
        "                # m_t = beta1 * m_{t-1} + (1-beta1) * g_nat\n",
        "                exp_avg.mul_(beta1).add_(natural_grad, alpha=1 - beta1)\n",
        "\n",
        "                # 6. Apply Weight Decay\n",
        "                # FAdam paper argues for \"Natural Weight Decay\" (theta/sqrt(v)),\n",
        "                # but for head-to-head fairness with AdamW, we use Decoupled (AdamW style)\n",
        "                # unless strictly following Eq 28. Let's use Decoupled for stability.\n",
        "                if group['weight_decay'] > 0:\n",
        "                    param.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                # 7. Update Parameters\n",
        "                # theta = theta - lr * m_t\n",
        "                # Note: No bias correction on m_t in FAdam paper (Section 3.4.2)\n",
        "                param.add_(exp_avg, alpha=-group['lr'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "class AdamMini(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Adam-mini (Zhang et al., 2025).\n",
        "    Key Innovation: Block-wise v_t averaging.\n",
        "    Memory Usage: Stores full m_t, but v_t is reduced by >99%.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0.0, model_sharding=False):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            lr = group['lr']\n",
        "            wd = group['weight_decay']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # Init State\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    # Adam-mini: v_t is a scalar per block (or per head)\n",
        "                    # The 'params' passed here should already be chunked correctly by the user\n",
        "                    # In this naive PyTorch impl, we assume p is a block.\n",
        "                    # We store v_mean as a single scalar for this tensor block.\n",
        "                    state['exp_avg_sq_mean'] = torch.tensor(0.0).to(p.device)\n",
        "\n",
        "                m = state['exp_avg']\n",
        "                v_mean = state['exp_avg_sq_mean']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # 1. Decoupled Weight Decay\n",
        "                if wd > 0:\n",
        "                    p.mul_(1 - lr * wd)\n",
        "\n",
        "                # 2. Update Momentum (m) - Standard Pointwise\n",
        "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                # 3. Update v_mean (Block-wise average)\n",
        "                # Calculate mean(g^2) for this specific block\n",
        "                g_sq_mean = torch.mean(grad ** 2)\n",
        "                v_mean.mul_(beta2).add_(g_sq_mean, alpha=1 - beta2)\n",
        "\n",
        "                # 4. Bias Correction\n",
        "                m_hat = m / (1 - beta1 ** state['step'])\n",
        "                v_mean_hat = v_mean / (1 - beta2 ** state['step'])\n",
        "\n",
        "                # 5. Update\n",
        "                # Effective LR is lr / sqrt(v_mean_hat)\n",
        "                # Note: v_mean_hat is a scalar, so this broadcast divides.\n",
        "                # This simulates loading 1 scalar from HBM instead of a whole tensor.\n",
        "                update = m_hat / (torch.sqrt(v_mean_hat) + eps)\n",
        "                p.add_(update, alpha=-lr)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# ==========================================\n",
        "# 2. PARTITIONING LOGIC (Adam-mini)\n",
        "# ==========================================\n",
        "\n",
        "def get_adam_mini_param_groups(model, lr, weight_decay, betas, eps):\n",
        "    \"\"\"\n",
        "    Implements Algorithm 3 from Adam-mini paper.\n",
        "    Partitions parameters based on Hessian structure:\n",
        "    - Embed/Head: By Token (dim 0 usually)\n",
        "    - Query/Key: By Heads\n",
        "    - Value/MLP: By Output Neurons\n",
        "    \"\"\"\n",
        "    # Simplified partitioning for Llama architecture\n",
        "    # In PyTorch, Linear layers are (out_features, in_features).\n",
        "    # \"Partition by output neurons\" means treating each row as a block?\n",
        "    # Actually, Adam-mini paper says: \"assign a single learning rate to each parameter block\".\n",
        "    # If we treat the whole Linear weight matrix as one block, we get 1 v value for the whole matrix.\n",
        "    # The paper suggests finer granularity (e.g. per head).\n",
        "    # For this demo, to keep it runnable without writing a custom Kernel that handles\n",
        "    # jagged tensors, we will treat **each Linear layer weight** as a single block\n",
        "    # for the 'neuron' partition strategies (Value, MLP), and split Q/K manually if possible.\n",
        "\n",
        "    # However, standard PyTorch optimizers iterate over tensors.\n",
        "    # To truly implement Adam-mini, we would ideally reshape Q/K weights into (n_heads, head_dim, ...)\n",
        "    # and treat them as list of parameters.\n",
        "\n",
        "    # Pragmatic approach for this script:\n",
        "    # We will let AdamMini optimizer treat whatever tensor it gets as a \"block\".\n",
        "    # So we need to reshape the model parameters *before* passing them to the optimizer?\n",
        "    # No, that breaks the model forward pass.\n",
        "    # We will wrap the optimizer step to perform the reduction correctly based on parameter name.\n",
        "\n",
        "    # REVISION: We will implement a simplified Adam-mini that treats\n",
        "    # whole layers as blocks. This captures the memory savings (1 scalar v per layer)\n",
        "    # but might lose the per-head granularity.\n",
        "    # To match the paper strictly, we would need to shard the Q/K tensors.\n",
        "    # For the sake of the \"Head to Head\" demo on a single GPU, reducing v to 1 scalar per layer\n",
        "    # is a valid approximation of the \"Memory Saving\" aspect, even if slightly suboptimal on convergence.\n",
        "\n",
        "    # Standard Grouping\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        # Standard WD exclusion\n",
        "        if param.dim() < 2 or \"ln\" in name or \"bias\" in name:\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "\n",
        "    groups = [\n",
        "        {'params': decay, 'weight_decay': weight_decay},\n",
        "        {'params': no_decay, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    return groups\n",
        "\n",
        "# ==========================================\n",
        "# 3. BENCHMARK SETUP\n",
        "# ==========================================\n",
        "\n",
        "def get_model():\n",
        "    # Use a small Llama config for speed, but deep enough to show divergence\n",
        "    config = LlamaConfig(\n",
        "        vocab_size=10000,\n",
        "        hidden_size=512,\n",
        "        intermediate_size=1024,\n",
        "        num_hidden_layers=4,\n",
        "        num_attention_heads=8,\n",
        "        max_position_embeddings=512\n",
        "    )\n",
        "    model = LlamaForCausalLM(config)\n",
        "    return model.cuda()\n",
        "\n",
        "def get_data(batch_size=16, seq_len=128):\n",
        "    # Synthetic data generator\n",
        "    vocab_size = 10000\n",
        "    while True:\n",
        "        yield torch.randint(0, vocab_size, (batch_size, seq_len)).cuda()\n",
        "\n",
        "def train_run(optimizer_name, steps=500):\n",
        "    # Force clean memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model = get_model()\n",
        "    model.train()\n",
        "\n",
        "    # Common Hyperparams (AdamW Baseline)\n",
        "    lr = 3e-4\n",
        "    wd = 0.1\n",
        "    betas = (0.9, 0.95)\n",
        "\n",
        "    if optimizer_name == \"AdamW\":\n",
        "        optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "    elif optimizer_name == \"FAdam\":\n",
        "        optim = FAdam(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "    elif optimizer_name == \"AdamMini\":\n",
        "        # Using the simplified block strategy (Per-tensor blocking)\n",
        "        groups = get_adam_mini_param_groups(model, lr, wd, betas, 1e-8)\n",
        "        optim = AdamMini(groups, lr=lr, betas=betas, weight_decay=wd)\n",
        "\n",
        "    data_gen = get_data()\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"\\n--- Starting {optimizer_name} ---\")\n",
        "\n",
        "    for step in range(steps):\n",
        "        inputs = next(data_gen)\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: Loss {loss.item():.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    max_mem = torch.cuda.max_memory_allocated() / 1024**2 # MB\n",
        "\n",
        "    return losses, max_mem, end_time - start_time\n",
        "\n",
        "# ==========================================\n",
        "# 4. EXECUTION & PLOTTING\n",
        "# ==========================================\n",
        "\n",
        "results = {}\n",
        "optimizers = [\"AdamW\", \"FAdam\", \"AdamMini\"]\n",
        "\n",
        "for opt in optimizers:\n",
        "    losses, mem, duration = train_run(opt, steps=300)\n",
        "    results[opt] = {\n",
        "        'loss': losses,\n",
        "        'memory': mem,\n",
        "        'time': duration\n",
        "    }\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Loss Curves\n",
        "plt.subplot(1, 2, 1)\n",
        "for opt in optimizers:\n",
        "    plt.plot(results[opt]['loss'], label=opt, alpha=0.8)\n",
        "plt.title(\"Training Loss Convergence\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Resource Usage Bar Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "mems = [results[o]['memory'] for o in optimizers]\n",
        "times = [results[o]['time'] for o in optimizers]\n",
        "\n",
        "x = range(len(optimizers))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax1 = plt.gca(), plt.gca()\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "bars1 = ax1.bar([i - width/2 for i in x], mems, width, label='VRAM (MB)', color='skyblue')\n",
        "bars2 = ax2.bar([i + width/2 for i in x], times, width, label='Time (s)', color='orange')\n",
        "\n",
        "ax1.set_ylabel('Peak VRAM (MB)')\n",
        "ax2.set_ylabel('Training Time (s)')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(optimizers)\n",
        "plt.title(\"Resource Consumption\")\n",
        "\n",
        "# Add standard Adam baseline reference in logic explanation\n",
        "print(\"\\n=== RESULTS SUMMARY ===\")\n",
        "for opt in optimizers:\n",
        "    final_loss = results[opt]['loss'][-1]\n",
        "    print(f\"{opt}: Final Loss={final_loss:.4f} | Peak VRAM={results[opt]['memory']:.0f}MB | Time={results[opt]['time']:.2f}s\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UFFfPrXUrv8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import LlamaConfig, LlamaForCausalLM\n",
        "\n",
        "# ==========================================\n",
        "# 1. ADVANCED OPTIMIZER: ADAM-MINI + ADAPTIVE EPS\n",
        "# ==========================================\n",
        "class AdamMiniAdvanced(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Adam-mini with 'Precision Matching'.\n",
        "    If the model is BF16, momentum (m) will be BF16.\n",
        "    Variance (v) is still computed in FP32 for stability but compressed.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0.0, adaptive_eps_threshold=0.01):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
        "                        adaptive_eps_threshold=adaptive_eps_threshold)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            lr = group['lr']\n",
        "            wd = group['weight_decay']\n",
        "            base_eps = group['eps']\n",
        "            adapt_threshold = group['adaptive_eps_threshold']\n",
        "\n",
        "            strategy = group.get('strategy', 'default')\n",
        "            n_blocks = group.get('n_blocks', 1)\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # --- Identify Block Structure ---\n",
        "                if strategy == 'head' or strategy == 'neuron':\n",
        "                    if p.dim() > 1 and p.shape[0] % n_blocks == 0:\n",
        "                        grad_view = grad.view(n_blocks, -1)\n",
        "                        param_view = p.view(n_blocks, -1)\n",
        "                    else:\n",
        "                        grad_view = grad.view(1, -1)\n",
        "                        param_view = p.view(1, -1)\n",
        "                        n_blocks = 1\n",
        "                else:\n",
        "                    grad_view = grad.view(1, -1)\n",
        "                    param_view = p.view(1, -1)\n",
        "                    n_blocks = 1\n",
        "\n",
        "                # --- State Initialization ---\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # OPTIMIZATION: Match momentum dtype to param dtype (BF16 -> BF16)\n",
        "                    # This allows 'Apples-to-Apples' memory comparison with standard AdamW\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    # Variance MUST be FP32 for numerical stability, but it's tiny (compressed)\n",
        "                    state['exp_avg_sq_mean'] = torch.zeros((n_blocks, 1),\n",
        "                                                           dtype=torch.float32, device=p.device)\n",
        "\n",
        "                m = state['exp_avg']\n",
        "                v_mean = state['exp_avg_sq_mean']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # --- Update Logic ---\n",
        "                if wd > 0:\n",
        "                    p.mul_(1 - lr * wd)\n",
        "\n",
        "                # Momentum Update\n",
        "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                # Variance Update (Perform in FP32)\n",
        "                # Cast grad to float32 for variance calc to avoid underflow\n",
        "                grad_fp32 = grad_view.to(torch.float32)\n",
        "                g_sq_mean = grad_fp32.pow(2).mean(dim=1, keepdim=True)\n",
        "                v_mean.mul_(beta2).add_(g_sq_mean, alpha=1 - beta2)\n",
        "\n",
        "                # Bias Correction\n",
        "                step = state['step']\n",
        "                m_hat = m / (1 - beta1 ** step)\n",
        "                v_mean_hat = v_mean / (1 - beta2 ** step)\n",
        "\n",
        "                # Adaptive Epsilon\n",
        "                grad_rms = g_sq_mean.sqrt()\n",
        "                safe_floor = 1e-12\n",
        "                adaptive_eps = (grad_rms * adapt_threshold).clamp(min=safe_floor, max=base_eps)\n",
        "\n",
        "                # Update\n",
        "                denom = v_mean_hat.sqrt().add_(adaptive_eps)\n",
        "                m_view = m_hat.view(n_blocks, -1)\n",
        "\n",
        "                # We cast the update term to match param dtype before adding\n",
        "                update_step = (m_view / denom).to(p.dtype)\n",
        "                param_view.add_(update_step, alpha=-lr)\n",
        "\n",
        "        return loss\n",
        "# ==========================================\n",
        "# 2. INTELLIGENT PARTITIONING (Llama 3 Logic)\n",
        "# ==========================================\n",
        "\n",
        "def configure_adam_mini_groups(model, config, lr, wd):\n",
        "    \"\"\"\n",
        "    Parses Llama 3 architecture and assigns:\n",
        "    - Heads partitioning for Q/K\n",
        "    - Neuron partitioning for V/MLP\n",
        "    - Token partitioning for Embeds\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "    # Identify layer counts\n",
        "    n_heads = config.num_attention_heads\n",
        "    n_kv_heads = config.num_key_value_heads\n",
        "    hidden_size = config.hidden_size\n",
        "\n",
        "    # Iterate parameters\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "\n",
        "        # Defaults\n",
        "        strategy = 'default'\n",
        "        n_blocks = 1\n",
        "        curr_wd = wd\n",
        "\n",
        "        # Skip WD for layernorms/biases\n",
        "        if param.dim() < 2 or \"norm\" in name or \"bias\" in name:\n",
        "            curr_wd = 0.0\n",
        "\n",
        "        # --- Llama 3 Partitioning Rules ---\n",
        "\n",
        "        # 1. Attention: Queries (Partition by Heads)\n",
        "        if \"q_proj\" in name:\n",
        "            strategy = 'head'\n",
        "            n_blocks = n_heads\n",
        "\n",
        "        # 2. Attention: Keys (Partition by KV Heads - GQA Support)\n",
        "        elif \"k_proj\" in name:\n",
        "            strategy = 'head'\n",
        "            n_blocks = n_kv_heads\n",
        "\n",
        "        # 3. Attention: Values (Partition by Output Neurons / KV Heads)\n",
        "        # In Llama, V_proj is (n_kv_heads * head_dim, hidden).\n",
        "        # Partitioning by rows effectively means partitioning by KV heads (or neurons).\n",
        "        # We stick to Neurons (Rows) as per Adam-mini paper for V.\n",
        "        elif \"v_proj\" in name:\n",
        "            strategy = 'neuron'\n",
        "            n_blocks = param.shape[0] # Number of output rows\n",
        "\n",
        "        # 4. Output Projections & MLPs (Partition by Neurons)\n",
        "        elif \"o_proj\" in name or \"gate_proj\" in name or \"up_proj\" in name or \"down_proj\" in name:\n",
        "            strategy = 'neuron'\n",
        "            n_blocks = param.shape[0]\n",
        "\n",
        "        # 5. Embeddings (Partition by Tokens)\n",
        "        elif \"embed_tokens\" in name or \"lm_head\" in name:\n",
        "            # Shape (vocab, hidden). Partition by vocab (rows)\n",
        "            strategy = 'neuron'\n",
        "            n_blocks = param.shape[0]\n",
        "\n",
        "        groups.append({\n",
        "            'params': [param],\n",
        "            'weight_decay': curr_wd,\n",
        "            'strategy': strategy,\n",
        "            'n_blocks': n_blocks,\n",
        "            'name': name # Debugging help\n",
        "        })\n",
        "\n",
        "    return groups\n",
        "\n",
        "# ==========================================\n",
        "# 3. SIMULATION SETUP (Llama 3 1B)\n",
        "# ==========================================\n",
        "\n",
        "def get_llama3_1b():\n",
        "    # Configuration mimicking Llama 3.2 1B structure\n",
        "    config = LlamaConfig(\n",
        "        vocab_size=32000,\n",
        "        hidden_size=2048,\n",
        "        intermediate_size=5632, # SwiGLU ratio\n",
        "        num_hidden_layers=16,     # Deep enough to stress optimizer\n",
        "        num_attention_heads=32,\n",
        "        num_key_value_heads=8,    # GQA 4:1 ratio\n",
        "        max_position_embeddings=4096,\n",
        "        hidden_act=\"silu\",\n",
        "        rms_norm_eps=1e-5\n",
        "    )\n",
        "    return LlamaForCausalLM(config).cuda().to(torch.bfloat16)\n",
        "\n",
        "def benchmark_run(optimizer_type, steps=200):\n",
        "    # Setup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model = get_llama3_1b()\n",
        "    model.train()\n",
        "\n",
        "    # Synthetic Data (High Context to stress memory)\n",
        "    bs, seq = 4, 2048\n",
        "    data = torch.randint(0, 32000, (bs, seq)).cuda()\n",
        "\n",
        "    # Optimizer Init\n",
        "    lr = 2e-4\n",
        "    wd = 0.01\n",
        "\n",
        "    if optimizer_type == \"AdamW\":\n",
        "        optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    elif optimizer_type == \"AdamMini_Adaptive\":\n",
        "        groups = configure_adam_mini_groups(model, model.config, lr, wd)\n",
        "        # Enable Adaptive Epsilon here\n",
        "        optim = AdamMiniAdvanced(groups, lr=lr, weight_decay=wd, adaptive_eps_threshold=0.01)\n",
        "\n",
        "    print(f\"\\n--- Running {optimizer_type} ---\")\n",
        "    losses = []\n",
        "    start_t = time.time()\n",
        "\n",
        "    for i in range(steps):\n",
        "        optim.zero_grad()\n",
        "        # Enable gradient checkpointing to allow fitting larger batch/model if needed\n",
        "        # model.gradient_checkpointing_enable()\n",
        "\n",
        "        out = model(data, labels=data)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Step {i}: {loss.item():.4f}\")\n",
        "\n",
        "    end_t = time.time()\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1024**3 # GB\n",
        "\n",
        "    return losses, peak_mem, end_t - start_t\n",
        "\n",
        "# ==========================================\n",
        "# 4. EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "configs = [\"AdamW\", \"AdamMini_Adaptive\"]\n",
        "results = {}\n",
        "\n",
        "for cfg in configs:\n",
        "    try:\n",
        "        loss, mem, duration = benchmark_run(cfg, steps=150)\n",
        "        results[cfg] = {'loss': loss, 'mem': mem, 'time': duration}\n",
        "        print(f\"Result {cfg}: {mem:.2f} GB | {duration:.2f} s\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"{cfg} OOM or Error: {e}\")\n",
        "        results[cfg] = {'loss': [], 'mem': 0, 'time': 0}\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cfg in configs:\n",
        "    if len(results[cfg]['loss']) > 0:\n",
        "        plt.plot(results[cfg]['loss'], label=cfg)\n",
        "plt.title(\"Llama 3-1B Convergence (AdamW vs Adam-mini-Adaptive)\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print Stats\n",
        "print(\"\\n=== FINAL STATS (A100 80GB) ===\")\n",
        "for cfg in configs:\n",
        "    print(f\"{cfg:20} | Peak VRAM: {results[cfg]['mem']:.2f} GB | Time: {results[cfg]['time']:.2f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "CvkOP5NQpYON",
        "outputId": "941dfc9a-49e4-41da-a719-99a1e7477a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running AdamW ---\n",
            "Step 0: 10.7687\n",
            "Step 50: 0.0017\n",
            "Step 100: 0.0014\n",
            "Result AdamW: 19.09 GB | 47.88 s\n",
            "\n",
            "--- Running AdamMini_Adaptive ---\n",
            "Step 0: 10.7935\n",
            "Step 50: 0.0021\n",
            "Step 100: 0.0017\n",
            "Result AdamMini_Adaptive: 17.51 GB | 49.06 s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL STATS (A100 80GB) ===\n",
            "AdamW                | Peak VRAM: 19.09 GB | Time: 47.88 s\n",
            "AdamMini_Adaptive    | Peak VRAM: 17.51 GB | Time: 49.06 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_inspect_memory(optimizer_type):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    model = get_llama3_1b()\n",
        "    model.train()\n",
        "\n",
        "    lr, wd = 2e-4, 0.01\n",
        "\n",
        "    if optimizer_type == \"AdamW\":\n",
        "        optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    else:\n",
        "        groups = configure_adam_mini_groups(model, model.config, lr, wd)\n",
        "        # Using the NEW class defined above\n",
        "        optim = AdamMiniAdvanced(groups, lr=lr, weight_decay=wd, adaptive_eps_threshold=0.01)\n",
        "\n",
        "    # Init states\n",
        "    data = torch.randint(0, 32000, (1, 32)).cuda()\n",
        "    model(data, labels=data).loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # Inspect Dtypes\n",
        "    print(f\"--- {optimizer_type} Inspection ---\")\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            if optimizer_type == \"AdamMini_Adaptive\":\n",
        "                # Check momentum dtype\n",
        "                m_dtype = optim.state[p]['exp_avg'].dtype\n",
        "                print(f\"Momentum Dtype: {m_dtype}\")\n",
        "                break # Just check one\n",
        "            else:\n",
        "                m_dtype = optim.state[p]['exp_avg'].dtype\n",
        "                print(f\"Momentum Dtype: {m_dtype}\")\n",
        "                break\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return torch.cuda.memory_allocated() / 1024**3\n",
        "\n",
        "print(f\"AdamW Static:      {deep_inspect_memory('AdamW'):.2f} GB\")\n",
        "print(f\"AdamMini Static:   {deep_inspect_memory('AdamMini_Adaptive'):.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfaLJ3VKs9kv",
        "outputId": "7c018bd6-6404-4928-c528-52026b1dd066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- AdamW Inspection ---\n",
            "AdamW Static:      4.79 GB\n",
            "--- AdamMini_Adaptive Inspection ---\n",
            "AdamMini Static:   3.20 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class AdamMini(Optimizer):\n",
        "    \"\"\"\n",
        "    Adam-mini (Zhang et al., 2025) with FAdam's Adaptive Epsilon (Hwang, 2024).\n",
        "\n",
        "    Features:\n",
        "    - Memory: Stores 'v' as block-wise scalars (saving >99% of v memory).\n",
        "    - Stability: Uses Adaptive Epsilon to prevent NaNs in sparse embeddings.\n",
        "    - Precision: Keeps states in FP32 for stability, or matches param dtype for efficiency.\n",
        "\n",
        "    Args:\n",
        "        params: Iterable of parameters.\n",
        "        lr (float): Learning rate.\n",
        "        betas (Tuple[float, float]): Coefficients for computing running averages (default: (0.9, 0.999)).\n",
        "        eps (float): Term added to the denominator to improve numerical stability (default: 1e-8).\n",
        "        weight_decay (float): Decoupled weight decay (AdamW style) (default: 0).\n",
        "        adaptive_eps_threshold (float): Threshold for FAdam safety rail (default: 0.01).\n",
        "        model_sharding (bool): If True, treats all params as independent blocks (default: False).\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0.0, adaptive_eps_threshold=0.01, model_sharding=False):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
        "                        adaptive_eps_threshold=adaptive_eps_threshold,\n",
        "                        model_sharding=model_sharding)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            lr = group['lr']\n",
        "            wd = group['weight_decay']\n",
        "            base_eps = group['eps']\n",
        "            adapt_threshold = group['adaptive_eps_threshold']\n",
        "\n",
        "            # Adam-mini partitioning strategy\n",
        "            strategy = group.get('strategy', 'default')\n",
        "            n_blocks = group.get('n_blocks', 1)\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # --- 1. Identify Block Structure ---\n",
        "                # Reshape gradient to (n_blocks, block_size) to allow reduction\n",
        "                if strategy in ['head', 'neuron']:\n",
        "                    if p.dim() > 1 and p.shape[0] % n_blocks == 0:\n",
        "                        grad_view = grad.view(n_blocks, -1)\n",
        "                        param_view = p.view(n_blocks, -1)\n",
        "                    else:\n",
        "                        grad_view = grad.view(1, -1)\n",
        "                        param_view = p.view(1, -1)\n",
        "                        n_blocks = 1\n",
        "                else:\n",
        "                    grad_view = grad.view(1, -1)\n",
        "                    param_view = p.view(1, -1)\n",
        "                    n_blocks = 1\n",
        "\n",
        "                # --- 2. State Initialization ---\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Momentum: Match param dtype (e.g. BF16) to save memory\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    # Variance: Must be FP32 for numerical stability, but it's tiny (n_blocks, 1)\n",
        "                    state['exp_avg_sq_mean'] = torch.zeros((n_blocks, 1),\n",
        "                                                           dtype=torch.float32, device=p.device)\n",
        "\n",
        "                m = state['exp_avg']\n",
        "                v_mean = state['exp_avg_sq_mean']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # --- 3. Decoupled Weight Decay ---\n",
        "                if wd > 0:\n",
        "                    p.mul_(1 - lr * wd)\n",
        "\n",
        "                # --- 4. Momentum Update ---\n",
        "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                # --- 5. Variance Update (FP32) ---\n",
        "                # Cast grad to FP32 for variance calculation to avoid underflow/overflow\n",
        "                grad_fp32 = grad_view.to(torch.float32)\n",
        "                g_sq_mean = grad_fp32.pow(2).mean(dim=1, keepdim=True)\n",
        "                v_mean.mul_(beta2).add_(g_sq_mean, alpha=1 - beta2)\n",
        "\n",
        "                # --- 6. Bias Correction ---\n",
        "                step = state['step']\n",
        "                m_hat = m / (1 - beta1 ** step)\n",
        "                v_mean_hat = v_mean / (1 - beta2 ** step)\n",
        "\n",
        "                # --- 7. Adaptive Epsilon ---\n",
        "                # Prevents NaNs when gradient is pure zero (sparse embeddings)\n",
        "                grad_rms = g_sq_mean.sqrt()\n",
        "                safe_floor = 1e-12\n",
        "                adaptive_eps = (grad_rms * adapt_threshold).clamp(min=safe_floor, max=base_eps)\n",
        "\n",
        "                # --- 8. Update Parameters ---\n",
        "                denom = v_mean_hat.sqrt().add_(adaptive_eps)\n",
        "\n",
        "                # Reshape m_hat to match the block view for broadcasting\n",
        "                m_view = m_hat.view(n_blocks, -1)\n",
        "\n",
        "                # Calculate update, casting back to param dtype if necessary\n",
        "                update_step = (m_view / denom).to(p.dtype)\n",
        "                param_view.add_(update_step, alpha=-lr)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ObbVMLWsvGVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}