{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPmyHG62Dq3r4QyiNK8PEl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df88dea6285e4d4d805910ca6f30528f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc971cbfc3ae4b6a890054837778db47",
              "IPY_MODEL_caaa156ba2b14a029847d22671137cfd",
              "IPY_MODEL_fa534c014f10463c8fe6d04650a0e5f2"
            ],
            "layout": "IPY_MODEL_5554338a13d147cdadbeca84015f040f"
          }
        },
        "dc971cbfc3ae4b6a890054837778db47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48f42621fc1043cc88127b666d40328e",
            "placeholder": "​",
            "style": "IPY_MODEL_cf5ca29975844357a442d5d6aa3a8aca",
            "value": "README.md: "
          }
        },
        "caaa156ba2b14a029847d22671137cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2778e05cab2f48c4890b536d38b90b0c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a4ca7100642431aa4a9761444d54872",
            "value": 1
          }
        },
        "fa534c014f10463c8fe6d04650a0e5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e39fcf042dea4d31b30c793fbbb0715a",
            "placeholder": "​",
            "style": "IPY_MODEL_5fa964950f5e43c49eb1505926d124ac",
            "value": " 10.5k/? [00:00&lt;00:00, 987kB/s]"
          }
        },
        "5554338a13d147cdadbeca84015f040f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f42621fc1043cc88127b666d40328e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf5ca29975844357a442d5d6aa3a8aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2778e05cab2f48c4890b536d38b90b0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9a4ca7100642431aa4a9761444d54872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e39fcf042dea4d31b30c793fbbb0715a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa964950f5e43c49eb1505926d124ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74fab9c4805348879e6bae11ce6d0d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb1996d20ed847e193eb66d045c86a86",
              "IPY_MODEL_e92cce408d1b4d989969a24bcf826d74",
              "IPY_MODEL_addc248892c44b05854fa51952fafd0e"
            ],
            "layout": "IPY_MODEL_9bb6d1a41b3743499e3ed887f0914463"
          }
        },
        "bb1996d20ed847e193eb66d045c86a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93a98977ca74e908bd0b1f291bffdad",
            "placeholder": "​",
            "style": "IPY_MODEL_2d4f3e4bca2c49d3a52c21b41e5a60a6",
            "value": "wikitext-103-raw-v1/test-00000-of-00001.(…): 100%"
          }
        },
        "e92cce408d1b4d989969a24bcf826d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752cde32cf2640f2ac37cca303b54958",
            "max": 732610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0c8d216abd24a22808f99fe17dc7f3c",
            "value": 732610
          }
        },
        "addc248892c44b05854fa51952fafd0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0ef1643e89413a99c640ac4d2f5707",
            "placeholder": "​",
            "style": "IPY_MODEL_ef686a670ccb4b7ba6298de28438b353",
            "value": " 733k/733k [00:01&lt;00:00, 62.4kB/s]"
          }
        },
        "9bb6d1a41b3743499e3ed887f0914463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e93a98977ca74e908bd0b1f291bffdad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4f3e4bca2c49d3a52c21b41e5a60a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "752cde32cf2640f2ac37cca303b54958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c8d216abd24a22808f99fe17dc7f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c0ef1643e89413a99c640ac4d2f5707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef686a670ccb4b7ba6298de28438b353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "386982e4e51d43c58d083cd2c8f55b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9518c606e7fd41749ce6596e59588dba",
              "IPY_MODEL_42cf86921b01406f9e86883e92dc3445",
              "IPY_MODEL_2d569008039f47a6b2ffeb83d35fa5ed"
            ],
            "layout": "IPY_MODEL_948aa484093e49b6b229fa47e22980a3"
          }
        },
        "9518c606e7fd41749ce6596e59588dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a93a066e2d34af283e767c199476a56",
            "placeholder": "​",
            "style": "IPY_MODEL_73432eb2fd5a48f398b062cde6cbf58d",
            "value": "wikitext-103-raw-v1/train-00000-of-00002(…): 100%"
          }
        },
        "42cf86921b01406f9e86883e92dc3445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a71295b29254eeea0b550a0d088fde6",
            "max": 156987808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd4ee7c795334b119a28f3dff1ca8d0c",
            "value": 156987808
          }
        },
        "2d569008039f47a6b2ffeb83d35fa5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_142cf8d544bc468685646ccbc50dd894",
            "placeholder": "​",
            "style": "IPY_MODEL_698b992faf9c4861b8fd8b4b00dabcd7",
            "value": " 157M/157M [00:02&lt;00:00, 133MB/s]"
          }
        },
        "948aa484093e49b6b229fa47e22980a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a93a066e2d34af283e767c199476a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73432eb2fd5a48f398b062cde6cbf58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a71295b29254eeea0b550a0d088fde6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4ee7c795334b119a28f3dff1ca8d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "142cf8d544bc468685646ccbc50dd894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698b992faf9c4861b8fd8b4b00dabcd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e947d41b1e794d48ba9dfde8ea8fa9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c54d6bd389ae4b2ba8dca6a2494683b7",
              "IPY_MODEL_5fd8fdef122f4c76b5f490a4d698305e",
              "IPY_MODEL_be565eb75b1141efbfc3e92cbdb6879f"
            ],
            "layout": "IPY_MODEL_547597b0f3194fefa36dda5c0c7f4d61"
          }
        },
        "c54d6bd389ae4b2ba8dca6a2494683b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08ec850d0574eac839ad77a09a98337",
            "placeholder": "​",
            "style": "IPY_MODEL_7e964d8fb30947c1b5bb02639826f2be",
            "value": "wikitext-103-raw-v1/train-00001-of-00002(…): 100%"
          }
        },
        "5fd8fdef122f4c76b5f490a4d698305e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a5dd957bb7404f982632454e8798e7",
            "max": 157088770,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e47c44fe38294729b2bca49679017bde",
            "value": 157088770
          }
        },
        "be565eb75b1141efbfc3e92cbdb6879f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee106c34bc4e40d5825e4af6cec4af02",
            "placeholder": "​",
            "style": "IPY_MODEL_b883507fd9dc429494d63358709388c1",
            "value": " 157M/157M [00:01&lt;00:00, 133MB/s]"
          }
        },
        "547597b0f3194fefa36dda5c0c7f4d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08ec850d0574eac839ad77a09a98337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e964d8fb30947c1b5bb02639826f2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1a5dd957bb7404f982632454e8798e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e47c44fe38294729b2bca49679017bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee106c34bc4e40d5825e4af6cec4af02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b883507fd9dc429494d63358709388c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb92b160850f4115bf06a7e16fd2fda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12212d2436fe4e71b4fa28ac93a3bdcf",
              "IPY_MODEL_628ee12b1282465cbc4b38ec9003a12a",
              "IPY_MODEL_40caa092d377480baa14641fa67ca0be"
            ],
            "layout": "IPY_MODEL_4aa4e59857254846ae53c2d783871fb5"
          }
        },
        "12212d2436fe4e71b4fa28ac93a3bdcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a07e1275f04e492e9e2d8e1b9f3b619f",
            "placeholder": "​",
            "style": "IPY_MODEL_b4b543b4ba7d4e12a9cef23026d22362",
            "value": "wikitext-103-raw-v1/validation-00000-of-(…): 100%"
          }
        },
        "628ee12b1282465cbc4b38ec9003a12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_290bc9874a1a47fe821ca787331d09f3",
            "max": 657209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dae02565e9a44ed1a113163d9c67440d",
            "value": 657209
          }
        },
        "40caa092d377480baa14641fa67ca0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00723a371f8e4c569f301e57d6cd79c7",
            "placeholder": "​",
            "style": "IPY_MODEL_5166237b62be443b81816137f85184c7",
            "value": " 657k/657k [00:00&lt;00:00, 74.7kB/s]"
          }
        },
        "4aa4e59857254846ae53c2d783871fb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07e1275f04e492e9e2d8e1b9f3b619f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b543b4ba7d4e12a9cef23026d22362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "290bc9874a1a47fe821ca787331d09f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dae02565e9a44ed1a113163d9c67440d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00723a371f8e4c569f301e57d6cd79c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5166237b62be443b81816137f85184c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "724586c79dd447b68cb5ada52fe31492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97fe260de3a241099de6e28d1c10017a",
              "IPY_MODEL_d302e80c9b0a4f9392757bd1d8e153c1",
              "IPY_MODEL_53078b1b75ba4f4e862859d500302b00"
            ],
            "layout": "IPY_MODEL_82a310255b98435089379e512c1dfa8f"
          }
        },
        "97fe260de3a241099de6e28d1c10017a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_722b3986786f473ab471b7527bf58f9e",
            "placeholder": "​",
            "style": "IPY_MODEL_c489f2d25d1442a385549db62e42d80a",
            "value": "Generating test split: 100%"
          }
        },
        "d302e80c9b0a4f9392757bd1d8e153c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ee7832d1be41519dc673418574c780",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_374fefdb25e04f43a80f6b8892b374d8",
            "value": 4358
          }
        },
        "53078b1b75ba4f4e862859d500302b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8f42485313b4800bddc64a4db3e0cb0",
            "placeholder": "​",
            "style": "IPY_MODEL_7cd9a18bd17f47a1afd022fbcf948625",
            "value": " 4358/4358 [00:00&lt;00:00, 98277.22 examples/s]"
          }
        },
        "82a310255b98435089379e512c1dfa8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722b3986786f473ab471b7527bf58f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c489f2d25d1442a385549db62e42d80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8ee7832d1be41519dc673418574c780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374fefdb25e04f43a80f6b8892b374d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8f42485313b4800bddc64a4db3e0cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd9a18bd17f47a1afd022fbcf948625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8572a57ddea477e8869761f2bd198f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d21444816f64ccda62aa805f8c9ebab",
              "IPY_MODEL_d4bab62ddfb64804a48d89fb6625cc1d",
              "IPY_MODEL_c47a0d5d29c74d599914014a00e339bc"
            ],
            "layout": "IPY_MODEL_c21a11051cd849988dd71dbee91f748b"
          }
        },
        "7d21444816f64ccda62aa805f8c9ebab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e2c693e3114884a9c25db811b8f891",
            "placeholder": "​",
            "style": "IPY_MODEL_bc83b8e2f3614890af91f4d93969bae6",
            "value": "Generating train split: 100%"
          }
        },
        "d4bab62ddfb64804a48d89fb6625cc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b2aa6a57684f9db2da477679784e55",
            "max": 1801350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de0fbbe94da54a6f951c83b339ae5fda",
            "value": 1801350
          }
        },
        "c47a0d5d29c74d599914014a00e339bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84f20124bcb2419c8e9faec8b2f39bdb",
            "placeholder": "​",
            "style": "IPY_MODEL_0234eea9208a49879f29974d00fbde38",
            "value": " 1801350/1801350 [00:02&lt;00:00, 918107.72 examples/s]"
          }
        },
        "c21a11051cd849988dd71dbee91f748b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e2c693e3114884a9c25db811b8f891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc83b8e2f3614890af91f4d93969bae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3b2aa6a57684f9db2da477679784e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de0fbbe94da54a6f951c83b339ae5fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84f20124bcb2419c8e9faec8b2f39bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0234eea9208a49879f29974d00fbde38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47ec08b049e144129cc0cf9aed1c723a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1d9872c47a8457c9c28ffc16de95564",
              "IPY_MODEL_b08cb62f6610441780236e7d905ed0de",
              "IPY_MODEL_a689e8fd67ea4ccf861fd02cc3ba4bab"
            ],
            "layout": "IPY_MODEL_a19ca5ad7cd74d5a8423c1f4b9cd8dd4"
          }
        },
        "b1d9872c47a8457c9c28ffc16de95564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30138db5aa494658aa95cd9f46bf05d7",
            "placeholder": "​",
            "style": "IPY_MODEL_6f227cccbd50478b82c6a3c232d6ee8b",
            "value": "Generating validation split: 100%"
          }
        },
        "b08cb62f6610441780236e7d905ed0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d740e4b3714248a9a381fe91274e91",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4e2fc2c08e44717a78ad302a1c0c80b",
            "value": 3760
          }
        },
        "a689e8fd67ea4ccf861fd02cc3ba4bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8fe82d61fc4f62899e9cb3c9c731f2",
            "placeholder": "​",
            "style": "IPY_MODEL_5962d80478074996a006f60ca78791cc",
            "value": " 3760/3760 [00:00&lt;00:00, 249581.93 examples/s]"
          }
        },
        "a19ca5ad7cd74d5a8423c1f4b9cd8dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30138db5aa494658aa95cd9f46bf05d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f227cccbd50478b82c6a3c232d6ee8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47d740e4b3714248a9a381fe91274e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e2fc2c08e44717a78ad302a1c0c80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a8fe82d61fc4f62899e9cb3c9c731f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5962d80478074996a006f60ca78791cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TesterSim2/The-Kepler-Architecture/blob/main/Untitled32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJvFbn-G9Qso",
        "outputId": "ecbbcbd2-2ba7-42d9-c20a-639d7ead5eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Diff-MLA Model...\n",
            "Parameters: 3.07M\n",
            "\n",
            "--- Training Speed Test (100 Steps) ---\n",
            "Step 0: Loss 4.4937\n",
            "Step 20: Loss 3.3118\n",
            "Step 40: Loss 3.2983\n",
            "Step 60: Loss 3.3686\n",
            "Step 80: Loss 3.3584\n",
            "Training Throughput: 169805 tokens/sec\n",
            "Peak VRAM: 3842 MB\n",
            "\n",
            "--- Inference Memory Test (KV Cache) ---\n",
            "Simulated Context: 8192 tokens\n",
            "Standard MHA Cache: 16.00 MB\n",
            "Diff-MLA Cache:     2.50 MB\n",
            "Compression Ratio:  6.40x\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Research Script: Differential Multi-Head Latent Attention (Diff-MLA)\n",
        "# ============================================================\n",
        "# Hypothesis: We can achieve the \"Noise Cancellation\" of DiffAttn\n",
        "# while maintaining the \"KV Cache Compression\" of MLA.\n",
        "#\n",
        "# Mechanism:\n",
        "# 1. Store Compressed Latent KV (cKV).\n",
        "# 2. Project Latent Query (cQ) into 2x Heads (Signal Group, Noise Group).\n",
        "# 3. Project cKV into 2x \"Virtual\" Keys via absorbed matrix multiplication.\n",
        "# 4. Compute DiffScore = Softmax(Signal) - lambda * Softmax(Noise).\n",
        "# 5. Apply Headwise Norm (Critical for DiffAttn stability).\n",
        "# ============================================================\n",
        "\n",
        "import os, math, time, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Setup & Device\n",
        "# ----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "DTYPE = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Data (TinyShakespeare)\n",
        "# ----------------------------\n",
        "# We use character-level for speed/simplicity to focus on architectural mechanics\n",
        "if not os.path.exists('input.txt'):\n",
        "    os.system('wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = vocab_size\n",
        "    dim: int = 512\n",
        "    n_layers: int = 6\n",
        "    n_heads: int = 8        # Output heads (DiffAttn uses 2x internally)\n",
        "    max_seq_len: int = 1024\n",
        "\n",
        "    # Diff-MLA Specifics\n",
        "    kv_rank: int = 128      # Compression for KV\n",
        "    q_rank: int = 256       # Compression for Query\n",
        "    rope_head_dim: int = 32 # Decoupled RoPE dimension\n",
        "    diff_lambda_init: float = 0.8\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Utilities\n",
        "# ----------------------------\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        return x * torch.rsqrt(var + self.eps) * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
        "    t = torch.arange(end, device=device)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    # Standard RoPE application\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs = freqs_cis[:xq.shape[1]].view(1, xq.shape[1], 1, -1)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "# ----------------------------\n",
        "# 4) The Hybrid Architecture: Diff-MLA\n",
        "# ----------------------------\n",
        "class DiffMLA(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        # 1. Query Compression (Latent Q)\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "\n",
        "        # 2. Up-Projections (Signal vs Noise Pairs)\n",
        "        # We generate 2x heads (Group 1 and Group 2) from the same latent vector\n",
        "        self.W_UQ1 = nn.Linear(self.q_rank, self.nh * self.hd, bias=False) # Signal Q\n",
        "        self.W_UQ2 = nn.Linear(self.q_rank, self.nh * self.hd, bias=False) # Noise Q\n",
        "\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False) # RoPE Q (Shared base?)\n",
        "\n",
        "        # 3. Key-Value Compression (The Latent Vector)\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "\n",
        "        # 4. Virtual Key Up-Projections (The Absorption Trick)\n",
        "        # Instead of 1 Up-Projection, we have 2.\n",
        "        # But we never output [B, T, H, D]. We use these weights to transform Q.\n",
        "        self.W_UK1 = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False) # Signal K\n",
        "        self.W_UK2 = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False) # Noise K\n",
        "\n",
        "        # 5. Value Projection (Shared)\n",
        "        # DiffAttn usually shares V between the diff pair\n",
        "        self.W_UV = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "\n",
        "        # 6. RoPE Key (Shared)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "\n",
        "        # 7. Differential Components\n",
        "        self.lambda_init = cfg.diff_lambda_init\n",
        "        self.lambda_q1 = nn.Parameter(torch.randn(self.hd))\n",
        "        self.lambda_k1 = nn.Parameter(torch.randn(self.hd))\n",
        "        self.lambda_q2 = nn.Parameter(torch.randn(self.hd))\n",
        "        self.lambda_k2 = nn.Parameter(torch.randn(self.hd))\n",
        "\n",
        "        # Headwise Normalization (Apply to each head output individually)\n",
        "        self.diff_norm = RMSNorm(self.hd)\n",
        "\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def _get_lambda(self):\n",
        "        # Calculate learnable lambda\n",
        "        l1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1))\n",
        "        l2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2))\n",
        "        return l1 - l2 + self.lambda_init\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # --- Compression Phase ---\n",
        "        cQ = self.W_DQ(x) # [B, T, q_rank]\n",
        "        cKV = self.W_DKV(x) # [B, T, kv_rank]\n",
        "\n",
        "        # --- Generate Heads (Signal & Noise) ---\n",
        "        # Content Queries\n",
        "        q1 = self.W_UQ1(cQ).view(B, T, self.nh, self.hd)\n",
        "        q2 = self.W_UQ2(cQ).view(B, T, self.nh, self.hd)\n",
        "\n",
        "        # RoPE Queries & Keys\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)\n",
        "\n",
        "        # --- The Absorption Trick (Training Mode) ---\n",
        "        # For training, we can just expand the Keys efficiently using SDPA\n",
        "        # In inference, we would do the Q @ W_UK trick. Here we stick to PyTorch SDPA logic.\n",
        "\n",
        "        # Expand Virtual Keys\n",
        "        k1 = self.W_UK1(cKV).view(B, T, self.nh, self.hd)\n",
        "        k2 = self.W_UK2(cKV).view(B, T, self.nh, self.hd)\n",
        "        v = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "\n",
        "        # Combine Content + RoPE\n",
        "        # Note: We duplicate q_rope/k_rope for both Signal and Noise branches\n",
        "        # to ensure position awareness is identical\n",
        "        q1_full = torch.cat([q1, q_rope], dim=-1)\n",
        "        q2_full = torch.cat([q2, q_rope], dim=-1)\n",
        "        k1_full = torch.cat([k1, k_rope], dim=-1)\n",
        "        k2_full = torch.cat([k2, k_rope], dim=-1)\n",
        "\n",
        "        # --- Differential Attention ---\n",
        "        # Score 1 (Signal)\n",
        "        # Scaled Dot Product Attention (Manual Softmax to allow subtraction)\n",
        "        scale = 1.0 / math.sqrt(self.hd + self.rhd)\n",
        "\n",
        "        # [B, H, T, T]\n",
        "        scores1 = torch.matmul(q1_full.transpose(1, 2), k1_full.transpose(1, 2).transpose(-2, -1)) * scale\n",
        "        scores2 = torch.matmul(q2_full.transpose(1, 2), k2_full.transpose(1, 2).transpose(-2, -1)) * scale\n",
        "\n",
        "        # Causal Mask\n",
        "        mask = torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
        "        scores1 = scores1 + mask\n",
        "        scores2 = scores2 + mask\n",
        "\n",
        "        attn1 = torch.softmax(scores1, dim=-1)\n",
        "        attn2 = torch.softmax(scores2, dim=-1)\n",
        "\n",
        "        # Subtraction\n",
        "        lam = self._get_lambda()\n",
        "        diff_attn = attn1 - lam * attn2\n",
        "\n",
        "        # Aggregate Values\n",
        "        # [B, H, T, T] @ [B, H, T, D] -> [B, H, T, D]\n",
        "        y = torch.matmul(diff_attn, v.transpose(1, 2))\n",
        "\n",
        "        # --- Headwise Normalization ---\n",
        "        # DiffAttn requires normalizing EACH head output before concatenation\n",
        "        y = y.transpose(1, 2) # [B, T, H, D]\n",
        "        y = self.diff_norm(y) # Norm over D dimension\n",
        "        y = y * (1 - self.lambda_init) # Scaling for gradient stability\n",
        "\n",
        "        y = y.reshape(B, T, C)\n",
        "        return self.c_proj(y)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def prefill_cache(self, x, freqs_cis):\n",
        "        # Returns standard MLA-style compressed cache\n",
        "        # cKV: [B, T, kv_rank]\n",
        "        # k_rope: [B, T, rhd]\n",
        "        cKV = self.W_DKV(x)\n",
        "        k_rope = self.W_KR(x)\n",
        "\n",
        "        # We compute output just like forward\n",
        "        out = self.forward(x, freqs_cis)\n",
        "\n",
        "        return out, {\"cKV\": cKV, \"k_rope\": k_rope}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode_step(self, x, freqs_cis, cache):\n",
        "        # Here is where the Memory Efficiency shines.\n",
        "        # We use the TINY cKV cache to generate BOTH Signal and Noise keys on the fly.\n",
        "\n",
        "        # 1. Update Cache\n",
        "        cKV_new = self.W_DKV(x)\n",
        "        k_rope_new = self.W_KR(x)\n",
        "\n",
        "        cache['cKV'] = torch.cat([cache['cKV'], cKV_new], dim=1)\n",
        "        cache['k_rope'] = torch.cat([cache['k_rope'], k_rope_new], dim=1)\n",
        "\n",
        "        cKV = cache['cKV'] # [B, Seq, kv_rank]\n",
        "\n",
        "        # 2. Generate Current Query\n",
        "        cQ = self.W_DQ(x)\n",
        "        q1 = self.W_UQ1(cQ).view(1, 1, self.nh, self.hd)\n",
        "        q2 = self.W_UQ2(cQ).view(1, 1, self.nh, self.hd)\n",
        "        q_rope = self.W_QR(cQ).view(1, 1, self.nh, self.rhd)\n",
        "\n",
        "        # 3. RoPE Rotation (Current Query + All Cached RoPE Keys)\n",
        "        # Note: In real impl, we'd rotate carefully. Here simplified for benchmark.\n",
        "        q_rope, _ = apply_rotary_emb(q_rope, q_rope, freqs_cis)\n",
        "\n",
        "        # 4. ABSORPTION TRICK: Generate Keys from Latent Cache\n",
        "        # Instead of storing K1/K2, we project cKV on the fly\n",
        "        # K1 = cKV @ W_UK1.T\n",
        "        k1 = self.W_UK1(cKV).view(1, -1, self.nh, self.hd)\n",
        "        k2 = self.W_UK2(cKV).view(1, -1, self.nh, self.hd)\n",
        "        v = self.W_UV(cKV).view(1, -1, self.nh, self.hd)\n",
        "\n",
        "        # 5. Compute Diff Attention\n",
        "        # (Simplified decode logic matching forward pass for benchmarking)\n",
        "        # In a real kernel, we would fuse the projection and dot product.\n",
        "        # Score = (Q @ W_UK.T) @ cKV.T\n",
        "\n",
        "        # ... (Standard logic follows, omitted for brevity in benchmark script logic)\n",
        "        # Returning dummy output to measure throughput structure\n",
        "        out = self.c_proj(torch.zeros_like(x))\n",
        "        return out, cache\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Training Loop\n",
        "# ----------------------------\n",
        "def get_batch(split_data):\n",
        "    ix = torch.randint(len(split_data) - 1024, (8,)) # batch 8, seq 1024\n",
        "    x = torch.stack([split_data[i:i+1024] for i in ix])\n",
        "    y = torch.stack([split_data[i+1:i+1024+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "def train_and_benchmark():\n",
        "    print(\"\\nInitializing Diff-MLA Model...\")\n",
        "    cfg = ModelConfig()\n",
        "    model = nn.Sequential(\n",
        "        nn.Embedding(cfg.vocab_size, cfg.dim),\n",
        "        DiffMLA(cfg), # The Star of the Show\n",
        "        DiffMLA(cfg), # Stack a few to feel the memory weight\n",
        "        DiffMLA(cfg),\n",
        "        RMSNorm(cfg.dim),\n",
        "        nn.Linear(cfg.dim, cfg.vocab_size, bias=False)\n",
        "    ).to(device)\n",
        "\n",
        "    # RoPE Cache\n",
        "    freqs = precompute_freqs_cis(cfg.rope_head_dim, 2048)\n",
        "\n",
        "    print(f\"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "    # 1. Training Speed Test\n",
        "    print(\"\\n--- Training Speed Test (100 Steps) ---\")\n",
        "    model.train()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i in range(100):\n",
        "        x, y = get_batch(data)\n",
        "\n",
        "        # Manual Forward for layers (to pass freqs)\n",
        "        h = model[0](x)\n",
        "        for layer in model[1:4]: # The 3 DiffMLA layers\n",
        "            h = layer(h, freqs[:x.shape[1]])\n",
        "        h = model[4](h)\n",
        "        logits = model[5](h)\n",
        "\n",
        "        loss = F.cross_entropy(logits.view(-1, cfg.vocab_size), y.view(-1))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if i % 20 == 0: print(f\"Step {i}: Loss {loss.item():.4f}\")\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Training Throughput: {100 * 8 * 1024 / dt:.0f} tokens/sec\")\n",
        "    print(f\"Peak VRAM: {torch.cuda.max_memory_allocated()/1024**2:.0f} MB\")\n",
        "\n",
        "    # 2. Inference Memory Test\n",
        "    print(\"\\n--- Inference Memory Test (KV Cache) ---\")\n",
        "    model.eval()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Simulate a long context generation (e.g., 4096 tokens)\n",
        "    # We will measure the size of the cache objects\n",
        "\n",
        "    # Create a dummy cache\n",
        "    # In Diff-MLA, we store [B, T, kv_rank] + [B, T, rope_dim]\n",
        "    # In MHA, we would store [B, T, n_heads, head_dim] * 2\n",
        "\n",
        "    batch = 1\n",
        "    seq_len = 8192 # Push it\n",
        "\n",
        "    # MLA Cache Size Calculation\n",
        "    # cKV: 1 * 8192 * 128 * 2 bytes (bf16)\n",
        "    # k_rope: 1 * 8192 * 32 * 2 bytes\n",
        "    mla_bytes = (batch * seq_len * cfg.kv_rank * 2) + (batch * seq_len * cfg.rope_head_dim * 2)\n",
        "\n",
        "    # MHA Equivalent\n",
        "    # K: 1 * 8192 * 8 * 64 * 2\n",
        "    # V: 1 * 8192 * 8 * 64 * 2\n",
        "    mha_bytes = (batch * seq_len * cfg.n_heads * (cfg.dim//cfg.n_heads) * 2) * 2\n",
        "\n",
        "    print(f\"Simulated Context: {seq_len} tokens\")\n",
        "    print(f\"Standard MHA Cache: {mha_bytes / 1024**2:.2f} MB\")\n",
        "    print(f\"Diff-MLA Cache:     {mla_bytes / 1024**2:.2f} MB\")\n",
        "    print(f\"Compression Ratio:  {mha_bytes / mla_bytes:.2f}x\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_benchmark()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Reproducibility\n",
        "# ==========================================\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = 65  # Will be set dynamically based on dataset\n",
        "    dim: int = 384        # Hidden dimension\n",
        "    n_layers: int = 6     # Depth\n",
        "    n_heads: int = 6      # Number of heads\n",
        "    max_seq_len: int = 1024\n",
        "    dropout: float = 0.0  # Zero dropout for pure architectural comparison\n",
        "\n",
        "    # Diff-MLA Specifics\n",
        "    # We tune these to keep parameter count roughly similar to MHA\n",
        "    kv_rank: int = 64     # High compression\n",
        "    q_rank: int = 128\n",
        "    rope_head_dim: int = 32\n",
        "    diff_lambda_init: float = 0.8\n",
        "\n",
        "def set_seed(seed=1337):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Shared Components (RoPE, Norms)\n",
        "# ==========================================\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        return x * torch.rsqrt(var + self.eps) * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
        "    t = torch.arange(end, device=device)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs = freqs_cis[:xq.shape[1]].view(1, xq.shape[1], 1, -1)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Model 1: Baseline MHA\n",
        "# ==========================================\n",
        "class CausalMHA(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_heads\n",
        "        self.head_dim = config.dim // config.n_heads\n",
        "        # Standard QKV projection: 3x dim -> 3x dim\n",
        "        self.c_attn = nn.Linear(config.dim, 3 * config.dim, bias=False)\n",
        "        self.c_proj = nn.Linear(config.dim, config.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "        q = q.view(B, T, self.n_head, self.head_dim)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim)\n",
        "\n",
        "        q, k = apply_rotary_emb(q, k, freqs_cis)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True\n",
        "        )\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.c_proj(y)\n",
        "\n",
        "# ==========================================\n",
        "# 4. Model 2: The Innovation (Diff-MLA)\n",
        "# ==========================================\n",
        "class DiffMLA(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        # 1. Compression\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "\n",
        "        # 2. Expansion (Signal vs Noise)\n",
        "        self.W_UQ1 = nn.Linear(self.q_rank, self.nh * self.hd, bias=False) # Signal Q\n",
        "        self.W_UQ2 = nn.Linear(self.q_rank, self.nh * self.hd, bias=False) # Noise Q\n",
        "        self.W_UK1 = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False) # Signal K\n",
        "        self.W_UK2 = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False) # Noise K\n",
        "        self.W_UV  = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False) # Shared V\n",
        "\n",
        "        # 3. RoPE (Decoupled Side-Channel)\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "\n",
        "        # 4. Differential Components\n",
        "        self.lambda_init = cfg.diff_lambda_init\n",
        "        self.lambda_q1 = nn.Parameter(torch.randn(self.hd))\n",
        "        self.lambda_k1 = nn.Parameter(torch.randn(self.hd))\n",
        "        self.lambda_q2 = nn.Parameter(torch.randn(self.hd))\n",
        "        self.lambda_k2 = nn.Parameter(torch.randn(self.hd))\n",
        "\n",
        "        self.diff_norm = RMSNorm(self.hd)\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def _get_lambda(self):\n",
        "        l1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1))\n",
        "        l2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2))\n",
        "        return l1 - l2 + self.lambda_init\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Compression\n",
        "        cQ = self.W_DQ(x)\n",
        "        cKV = self.W_DKV(x)\n",
        "\n",
        "        # Expansion\n",
        "        q1 = self.W_UQ1(cQ).view(B, T, self.nh, self.hd)\n",
        "        q2 = self.W_UQ2(cQ).view(B, T, self.nh, self.hd)\n",
        "        k1 = self.W_UK1(cKV).view(B, T, self.nh, self.hd)\n",
        "        k2 = self.W_UK2(cKV).view(B, T, self.nh, self.hd)\n",
        "        v  = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "\n",
        "        # RoPE Side-Channel\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)\n",
        "\n",
        "        # Combine Content + RoPE\n",
        "        q1_full = torch.cat([q1, q_rope], dim=-1)\n",
        "        q2_full = torch.cat([q2, q_rope], dim=-1)\n",
        "        k1_full = torch.cat([k1, k_rope], dim=-1)\n",
        "        k2_full = torch.cat([k2, k_rope], dim=-1)\n",
        "\n",
        "        # Differential Attention\n",
        "        scale = 1.0 / math.sqrt(self.hd + self.rhd)\n",
        "\n",
        "        # Compute raw scores\n",
        "        s1 = torch.matmul(q1_full.transpose(1, 2), k1_full.transpose(1, 2).transpose(-2, -1)) * scale\n",
        "        s2 = torch.matmul(q2_full.transpose(1, 2), k2_full.transpose(1, 2).transpose(-2, -1)) * scale\n",
        "\n",
        "        # Causal Masking\n",
        "        mask = torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
        "        s1 = s1 + mask\n",
        "        s2 = s2 + mask\n",
        "\n",
        "        # Subtraction\n",
        "        attn = torch.softmax(s1, dim=-1) - self._get_lambda() * torch.softmax(s2, dim=-1)\n",
        "\n",
        "        # Output\n",
        "        y = torch.matmul(attn, v.transpose(1, 2)) # [B, H, T, D]\n",
        "        y = self.diff_norm(y.transpose(1, 2))     # Headwise Norm\n",
        "        y = y * (1 - self.lambda_init)            # Gradient Scale\n",
        "\n",
        "        return self.c_proj(y.contiguous().view(B, T, C))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Generic Transformer Skeleton\n",
        "# ==========================================\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config: ModelConfig, model_type=\"mha\"):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.dim)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(config.n_layers):\n",
        "            self.layers.append(nn.ModuleDict({\n",
        "                'norm1': RMSNorm(config.dim),\n",
        "                'attn': DiffMLA(config) if model_type == \"diff_mla\" else CausalMHA(config),\n",
        "                'norm2': RMSNorm(config.dim),\n",
        "                'mlp': SwiGLU(config.dim, 4 * config.dim)\n",
        "            }))\n",
        "\n",
        "        self.final_norm = RMSNorm(config.dim)\n",
        "        self.lm_head = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_emb.weight # Weight tying\n",
        "\n",
        "        # Precompute RoPE table\n",
        "        # MHA uses head_dim, Diff-MLA uses rope_head_dim\n",
        "        dim_rope = config.rope_head_dim if model_type == \"diff_mla\" else config.dim // config.n_heads\n",
        "        self.freqs_cis = precompute_freqs_cis(dim_rope, config.max_seq_len * 2)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        x = self.token_emb(idx)\n",
        "        freqs = self.freqs_cis[:T]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # Attention Block\n",
        "            x = x + layer['attn'](layer['norm1'](x), freqs)\n",
        "            # MLP Block\n",
        "            x = x + layer['mlp'](layer['norm2'](x))\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# ==========================================\n",
        "# 6. Experimental Harness\n",
        "# ==========================================\n",
        "def train_experiment(model_type: str, steps: int = 1000):\n",
        "    print(f\"\\n--- Starting Experiment: {model_type.upper()} ---\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    if not os.path.exists('input.txt'):\n",
        "        os.system('wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f: text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
        "\n",
        "    # 2. Config & Init\n",
        "    config = ModelConfig(vocab_size=len(chars))\n",
        "    model = Transformer(config, model_type=model_type).to(device)\n",
        "\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Parameters: {param_count/1e6:.2f}M\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)\n",
        "\n",
        "    # 3. Training Loop\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    losses = []\n",
        "\n",
        "    for step in range(steps):\n",
        "        # Batching\n",
        "        ix = torch.randint(len(data) - config.max_seq_len, (16,)) # Batch size 16\n",
        "        x = torch.stack([data[i:i+config.max_seq_len] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+config.max_seq_len+1] for i in ix])\n",
        "\n",
        "        # Forward\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Step {step}: Loss {loss.item():.4f}\")\n",
        "\n",
        "    # 4. Final Metrics\n",
        "    torch.cuda.synchronize()\n",
        "    total_time = time.time() - start_time\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "    final_loss = sum(losses[-10:]) / 10\n",
        "\n",
        "    print(f\"Final Loss: {final_loss:.4f}\")\n",
        "    print(f\"Throughput: {steps * 16 * config.max_seq_len / total_time:.0f} tok/s\")\n",
        "    print(f\"Peak VRAM:  {peak_mem:.0f} MB\")\n",
        "\n",
        "    # Clean up\n",
        "    del model, optimizer, x, y\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    return final_loss, param_count, peak_mem\n",
        "\n",
        "# ==========================================\n",
        "# 7. Run Comparison\n",
        "# ==========================================\n",
        "print(\"Initializing Research Environment...\")\n",
        "set_seed(42)\n",
        "\n",
        "# Run Baseline\n",
        "loss_mha, params_mha, mem_mha = train_experiment(\"mha\", steps=1000)\n",
        "\n",
        "# Run Experimental\n",
        "loss_diff, params_diff, mem_diff = train_experiment(\"diff_mla\", steps=1000)\n",
        "\n",
        "print(\"\\n\\n========================================================\")\n",
        "print(f\"{'Metric':<15} | {'Baseline (MHA)':<15} | {'Diff-MLA':<15}\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(f\"{'Parameters':<15} | {params_mha/1e6:.2f}M            | {params_diff/1e6:.2f}M\")\n",
        "print(f\"{'Final Loss':<15} | {loss_mha:.4f}             | {loss_diff:.4f}\")\n",
        "print(f\"{'Peak VRAM':<15} | {mem_mha:.0f} MB             | {mem_diff:.0f} MB\")\n",
        "print(\"========================================================\")\n",
        "print(\"Interpretation:\")\n",
        "if loss_diff <= loss_mha:\n",
        "    print(\"SUCCESS: Diff-MLA matches or beats Baseline quality.\")\n",
        "else:\n",
        "    print(\"NOTE: Diff-MLA shows slight degradation (expected due to compression).\")\n",
        "print(\"Check VRAM usage. Training VRAM might be higher for Diff-MLA due to\")\n",
        "print(\"intermediate activations, but Inference Cache (KV) will be 6x smaller.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYKncsWo-k3j",
        "outputId": "7654e731-2f21-499e-bd83-286ec1e560f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "Initializing Research Environment...\n",
            "\n",
            "--- Starting Experiment: MHA ---\n",
            "Parameters: 14.19M\n",
            "Step 0: Loss 355.9028\n",
            "Step 200: Loss 2.3265\n",
            "Step 400: Loss 1.8679\n",
            "Step 600: Loss 1.6239\n",
            "Step 800: Loss 1.4711\n",
            "Final Loss: 1.4163\n",
            "Throughput: 137949 tok/s\n",
            "Peak VRAM:  4448 MB\n",
            "\n",
            "--- Starting Experiment: DIFF_MLA ---\n",
            "Parameters: 13.23M\n",
            "Step 0: Loss 347.7994\n",
            "Step 200: Loss 2.7611\n",
            "Step 400: Loss 2.1762\n",
            "Step 600: Loss 1.8915\n",
            "Step 800: Loss 1.6110\n",
            "Final Loss: 1.5788\n",
            "Throughput: 63190 tok/s\n",
            "Peak VRAM:  12721 MB\n",
            "\n",
            "\n",
            "========================================================\n",
            "Metric          | Baseline (MHA)  | Diff-MLA       \n",
            "--------------------------------------------------------\n",
            "Parameters      | 14.19M            | 13.23M\n",
            "Final Loss      | 1.4163             | 1.5788\n",
            "Peak VRAM       | 4448 MB             | 12721 MB\n",
            "========================================================\n",
            "Interpretation:\n",
            "NOTE: Diff-MLA shows slight degradation (expected due to compression).\n",
            "Check VRAM usage. Training VRAM might be higher for Diff-MLA due to\n",
            "intermediate activations, but Inference Cache (KV) will be 6x smaller.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, time, random\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Setup\n",
        "# ----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Data (TinyShakespeare)\n",
        "# ----------------------------\n",
        "if not os.path.exists('input.txt'):\n",
        "    os.system('wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "with open('input.txt', 'r', encoding='utf-8') as f: text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = vocab_size\n",
        "    dim: int = 512\n",
        "    n_layers: int = 6\n",
        "    n_heads: int = 8\n",
        "    max_seq_len: int = 1024\n",
        "\n",
        "    # MLA Specifics\n",
        "    kv_rank: int = 128\n",
        "    q_rank: int = 256\n",
        "    rope_head_dim: int = 32\n",
        "\n",
        "    # CMD-MLA Specifics\n",
        "    # We only use ONE noise head shared across all signal heads\n",
        "    diff_lambda_init: float = 0.8\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Utilities\n",
        "# ----------------------------\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        return x * torch.rsqrt(var + self.eps) * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
        "    t = torch.arange(end, device=device)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs = freqs_cis[:xq.shape[1]].view(1, xq.shape[1], 1, -1)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "# ----------------------------\n",
        "# 4) The Innovation: CMD-MLA\n",
        "# ----------------------------\n",
        "class CMD_MLA(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        # 1. Compression\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "\n",
        "        # 2. Signal Heads (Standard MLA)\n",
        "        self.W_UQ = nn.Linear(self.q_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UK = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UV = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "\n",
        "        # 3. The \"Common Mode\" Noise Head (SINGLE Head)\n",
        "        # It has its own projections from the SAME latent space\n",
        "        self.W_UQ_Noise = nn.Linear(self.q_rank, self.hd, bias=False)\n",
        "        self.W_UK_Noise = nn.Linear(self.kv_rank, self.hd, bias=False)\n",
        "\n",
        "        # 4. RoPE (Shared)\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "\n",
        "        # 5. Differential Components\n",
        "        self.lambda_init = cfg.diff_lambda_init\n",
        "        # Lambda is per-head: Each signal head decides how much noise to subtract\n",
        "        self.lambda_q = nn.Parameter(torch.randn(self.nh, self.hd))\n",
        "        self.lambda_k = nn.Parameter(torch.randn(self.nh, self.hd))\n",
        "\n",
        "        self.head_norm = RMSNorm(self.hd)\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Latents\n",
        "        cQ = self.W_DQ(x)\n",
        "        cKV = self.W_DKV(x)\n",
        "\n",
        "        # --- Signal Generation ---\n",
        "        q_sig = self.W_UQ(cQ).view(B, T, self.nh, self.hd) # [B, T, H, D]\n",
        "        k_sig = self.W_UK(cKV).view(B, T, self.nh, self.hd)\n",
        "        v_sig = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "\n",
        "        # --- Noise Generation (Single Head) ---\n",
        "        q_noise = self.W_UQ_Noise(cQ).view(B, T, 1, self.hd) # [B, T, 1, D]\n",
        "        k_noise = self.W_UK_Noise(cKV).view(B, T, 1, self.hd)\n",
        "\n",
        "        # --- RoPE ---\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)\n",
        "\n",
        "        # We attach RoPE to Signal only?\n",
        "        # Actually, DiffAttn paper suggests RoPE on both.\n",
        "        # But Noise should be positional too. Let's add RoPE to signal.\n",
        "        # For simplicity in this hypothesis, we assume Noise is content-heavy (stopwords).\n",
        "        # We attach RoPE to Signal.\n",
        "\n",
        "        q_sig_full = torch.cat([q_sig, q_rope], dim=-1)\n",
        "        k_sig_full = torch.cat([k_sig, k_rope], dim=-1)\n",
        "\n",
        "        # --- Attention Scores ---\n",
        "        scale = 1.0 / math.sqrt(self.hd + self.rhd)\n",
        "\n",
        "        # 1. Signal Scores [B, H, T, T]\n",
        "        scores_sig = torch.matmul(q_sig_full.transpose(1, 2), k_sig_full.transpose(1, 2).transpose(-2, -1)) * scale\n",
        "\n",
        "        # 2. Noise Scores [B, 1, T, T]\n",
        "        # Note: Noise head has no RoPE here to keep it \"Global/Background\" focused\n",
        "        scale_noise = 1.0 / math.sqrt(self.hd)\n",
        "        scores_noise = torch.matmul(q_noise.transpose(1, 2), k_noise.transpose(1, 2).transpose(-2, -1)) * scale_noise\n",
        "\n",
        "        # Causal Mask\n",
        "        mask = torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
        "        scores_sig = scores_sig + mask\n",
        "        scores_noise = scores_noise + mask\n",
        "\n",
        "        attn_sig = torch.softmax(scores_sig, dim=-1)\n",
        "        attn_noise = torch.softmax(scores_noise, dim=-1) # [B, 1, T, T]\n",
        "\n",
        "        # --- Differential Subtraction ---\n",
        "        # Lambda calculation per head\n",
        "        lam = torch.exp(torch.sum(self.lambda_q * self.lambda_k, dim=-1)) + self.lambda_init\n",
        "        lam = lam.view(1, self.nh, 1, 1) # Broadcast to batch and seq\n",
        "\n",
        "        # The Innovation: Subtract Global Noise from All Heads\n",
        "        # attn_sig: [B, H, T, T]\n",
        "        # attn_noise: [B, 1, T, T] -> Broadcasts to H\n",
        "        diff_attn = attn_sig - lam * attn_noise\n",
        "\n",
        "        # Value Projection\n",
        "        y = torch.matmul(diff_attn, v_sig.transpose(1, 2)) # [B, H, T, D]\n",
        "\n",
        "        # Norm & Output\n",
        "        y = self.head_norm(y.transpose(1, 2))\n",
        "        y = y * (1 - self.lambda_init)\n",
        "\n",
        "        return self.c_proj(y.contiguous().view(B, T, C))\n",
        "\n",
        "# ==========================================\n",
        "# 5. Standard MLA (Control Group)\n",
        "# ==========================================\n",
        "class MLA(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "        self.W_UQ = nn.Linear(self.q_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False)\n",
        "\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "        self.W_UK = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UV = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "        cQ = self.W_DQ(x)\n",
        "        q = self.W_UQ(cQ).view(B, T, self.nh, self.hd)\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "\n",
        "        cKV = self.W_DKV(x)\n",
        "        k = self.W_UK(cKV).view(B, T, self.nh, self.hd)\n",
        "        v = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "\n",
        "        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)\n",
        "\n",
        "        q_full = torch.cat([q, q_rope], dim=-1)\n",
        "        k_full = torch.cat([k, k_rope], dim=-1)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q_full.transpose(1, 2), k_full.transpose(1, 2), v.transpose(1, 2), is_causal=True\n",
        "        )\n",
        "        return self.c_proj(y.transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "# ==========================================\n",
        "# 6. Experimental Harness\n",
        "# ==========================================\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config: ModelConfig, model_type=\"mla\"):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.dim)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(config.n_layers):\n",
        "            self.layers.append(nn.ModuleDict({\n",
        "                'norm1': RMSNorm(config.dim),\n",
        "                'attn': CMD_MLA(config) if model_type == \"cmd_mla\" else MLA(config),\n",
        "                'norm2': RMSNorm(config.dim),\n",
        "                'mlp': SwiGLU(config.dim, 4 * config.dim)\n",
        "            }))\n",
        "        self.final_norm = RMSNorm(config.dim)\n",
        "        self.lm_head = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_emb.weight\n",
        "\n",
        "        dim_rope = config.rope_head_dim # Same for both\n",
        "        self.freqs_cis = precompute_freqs_cis(dim_rope, config.max_seq_len * 2)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        x = self.token_emb(idx)\n",
        "        freqs = self.freqs_cis[:T]\n",
        "        for layer in self.layers:\n",
        "            x = x + layer['attn'](layer['norm1'](x), freqs)\n",
        "            x = x + layer['mlp'](layer['norm2'](x))\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None if targets is None else F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "def run_experiment(name, model_type, steps=1000):\n",
        "    print(f\"\\n--- Experiment: {name} ---\")\n",
        "    cfg = ModelConfig(vocab_size=vocab_size)\n",
        "    model = Transformer(cfg, model_type=model_type).to(device)\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=steps)\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    t0 = time.time()\n",
        "    for step in range(steps):\n",
        "        ix = torch.randint(len(data) - cfg.max_seq_len, (16,))\n",
        "        x = torch.stack([data[i:i+cfg.max_seq_len] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+cfg.max_seq_len+1] for i in ix])\n",
        "\n",
        "        _, loss = model(x, y)\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        sched.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if step % 200 == 0: print(f\"Step {step}: {loss.item():.4f}\")\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    dt = time.time() - t0\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "    final_loss = sum(losses[-20:]) / 20\n",
        "\n",
        "    print(f\"Result: Loss {final_loss:.4f} | VRAM {peak_mem:.0f}MB | Time {dt:.1f}s\")\n",
        "\n",
        "    # Cleanup\n",
        "    del model, optim, x, y\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    return final_loss, peak_mem, dt\n",
        "\n",
        "# ==========================================\n",
        "# 7. Execution\n",
        "# ==========================================\n",
        "loss_mla, mem_mla, time_mla = run_experiment(\"Baseline MLA\", \"mla\")\n",
        "loss_cmd, mem_cmd, time_cmd = run_experiment(\"CMD-MLA (Ours)\", \"cmd_mla\")\n",
        "\n",
        "print(\"\\n=== Final Report ===\")\n",
        "print(f\"{'Metric':<10} | {'MLA':<10} | {'CMD-MLA':<10} | {'Delta'}\")\n",
        "print(\"-\" * 45)\n",
        "print(f\"{'Loss':<10} | {loss_mla:.4f}     | {loss_cmd:.4f}         | {loss_cmd - loss_mla:+.4f}\")\n",
        "print(f\"{'VRAM':<10} | {mem_mla:.0f}MB      | {mem_cmd:.0f}MB         | {mem_cmd - mem_mla:+.0f}MB\")\n",
        "print(f\"{'Time':<10} | {time_mla:.1f}s       | {time_cmd:.1f}s         | {time_cmd - time_mla:+.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM5Pu9wDCiGc",
        "outputId": "89458923-49de-490d-c322-a58ac4101e21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "--- Experiment: Baseline MLA ---\n",
            "Step 0: 484.7122\n",
            "Step 200: 3.0524\n",
            "Step 400: 2.4129\n",
            "Step 600: 2.0626\n",
            "Step 800: 1.9731\n",
            "Result: Loss 1.9352 | VRAM 5925MB | Time 196.7s\n",
            "\n",
            "--- Experiment: CMD-MLA (Ours) ---\n",
            "Step 0: 467.8169\n",
            "Step 200: 2.8713\n",
            "Step 400: 2.4459\n",
            "Step 600: 2.1647\n",
            "Step 800: 1.8582\n",
            "Result: Loss 1.7933 | VRAM 13471MB | Time 306.9s\n",
            "\n",
            "=== Final Report ===\n",
            "Metric     | MLA        | CMD-MLA    | Delta\n",
            "---------------------------------------------\n",
            "Loss       | 1.9352     | 1.7933         | -0.1420\n",
            "VRAM       | 5925MB      | 13471MB         | +7546MB\n",
            "Time       | 196.7s       | 306.9s         | +110.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Research Experiment: Scaled CMD-MLA (Common-Mode Differential Latent Attention)\n",
        "# Platform: Single A100 80GB (or 40GB)\n",
        "# Optimization: FlashAttention via (AV - BV) decomposition\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Setup & Dependencies\n",
        "# ------------------------------------------------------------------------------\n",
        "def setup_environment():\n",
        "    import subprocess, sys\n",
        "    def install(package):\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "    try: import datasets\n",
        "    except ImportError: install(\"datasets\")\n",
        "    try: import tiktoken\n",
        "    except ImportError: install(\"tiktoken\")\n",
        "\n",
        "setup_environment()\n",
        "from datasets import load_dataset\n",
        "import tiktoken\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=1337):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1337)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Enable TF32 for A100 Tensor Cores\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "print(f\"Compute Device: {device} ({torch.cuda.get_device_name(0) if device=='cuda' else 'CPU'})\")\n",
        "\n",
        "# 2. Data Pipeline (WikiText-103) - RAM Optimized\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"Loading WikiText-103...\")\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.encode(text) + [tokenizer.eot_token] for text in batch[\"text\"]]\n",
        "\n",
        "# We tokenize and flatten manually in batches to avoid RAM explosion\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "all_tokens = []\n",
        "batch_size = 1000  # Process 1k rows at a time\n",
        "print(\"Tokenizing and Flattening in streams...\")\n",
        "\n",
        "# Iterate through dataset without loading everything at once\n",
        "for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "    batch_text = dataset[i : i + batch_size]\n",
        "    # Tokenize\n",
        "    batch_ids = tokenize_batch(batch_text)\n",
        "    # Flatten list of lists efficiently\n",
        "    for seq in batch_ids:\n",
        "        all_tokens.extend(seq)\n",
        "\n",
        "print(\"Converting to Tensor...\")\n",
        "# Convert to tensor (uint16 is sufficient for vocab < 65535, saves 4x RAM)\n",
        "# We use int32/int64 for safety during training casting\n",
        "train_data = torch.tensor(all_tokens, dtype=torch.long)\n",
        "vocab_size = tokenizer.n_vocab\n",
        "\n",
        "print(f\"Dataset Loaded: {len(train_data):,} tokens. Vocab Size: {vocab_size}\")\n",
        "\n",
        "# Clean up CPU RAM\n",
        "del all_tokens, dataset\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# 3. Model Configuration (Scaled Up)\n",
        "# ------------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = vocab_size\n",
        "    dim: int = 1024           # GPT-2 Medium width\n",
        "    n_layers: int = 12        # Reasonable depth\n",
        "    n_heads: int = 16         # 64 dim per head\n",
        "    max_seq_len: int = 2048   # Longer context for A100\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    # CMD-MLA Specifics\n",
        "    kv_rank: int = 512        # Higher rank for larger model\n",
        "    q_rank: int = 1024        # Keep Q rank high for expressivity\n",
        "    rope_head_dim: int = 64\n",
        "    diff_lambda_init: float = 0.8\n",
        "\n",
        "# 4. Utilities\n",
        "# ------------------------------------------------------------------------------\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        return x * torch.rsqrt(var + self.eps) * self.weight\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
        "    t = torch.arange(end, device=device)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    # xq: [B, T, H, D]\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs = freqs_cis[:xq.shape[1]].view(1, xq.shape[1], 1, -1)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "# 5. Architecture: Optimized CMD-MLA\n",
        "# ------------------------------------------------------------------------------\n",
        "class Optimized_CMD_MLA(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        # Projections\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "\n",
        "        # Signal Heads\n",
        "        self.W_UQ = nn.Linear(self.q_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UK = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UV = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "\n",
        "        # Common Mode Noise Head (1 Head)\n",
        "        self.W_UQ_Noise = nn.Linear(self.q_rank, self.hd, bias=False)\n",
        "        self.W_UK_Noise = nn.Linear(self.kv_rank, self.hd, bias=False)\n",
        "\n",
        "        # RoPE\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "\n",
        "        # Differential Components\n",
        "        self.lambda_init = cfg.diff_lambda_init\n",
        "        self.lambda_q = nn.Parameter(torch.zeros(self.nh, self.hd)) # Init zero for stability\n",
        "        self.lambda_k = nn.Parameter(torch.zeros(self.nh, self.hd))\n",
        "\n",
        "        self.head_norm = RMSNorm(self.hd)\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # 1. Latents\n",
        "        cQ = self.W_DQ(x)\n",
        "        cKV = self.W_DKV(x)\n",
        "\n",
        "        # 2. Signal Generation\n",
        "        q_sig = self.W_UQ(cQ).view(B, T, self.nh, self.hd)\n",
        "        k_sig = self.W_UK(cKV).view(B, T, self.nh, self.hd)\n",
        "        v_sig = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "\n",
        "        # 3. Noise Generation (1 Head)\n",
        "        # We perform a trick here: We create 1 noise head, but we need to subtract it\n",
        "        # from H signal heads. To use FlashAttention, we expand Q/K noise to H heads.\n",
        "        # This effectively copies the noise computation H times, but allows parallel CUDA kernels.\n",
        "        q_noise = self.W_UQ_Noise(cQ).view(B, T, 1, self.hd).expand(B, T, self.nh, self.hd)\n",
        "        k_noise = self.W_UK_Noise(cKV).view(B, T, 1, self.hd).expand(B, T, self.nh, self.hd)\n",
        "\n",
        "        # 4. RoPE\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)\n",
        "\n",
        "        # Combine\n",
        "        q_sig_full = torch.cat([q_sig, q_rope], dim=-1)\n",
        "        k_sig_full = torch.cat([k_sig, k_rope], dim=-1)\n",
        "\n",
        "        # NOISE HEAD: No RoPE (Bag-of-words assumption for noise)\n",
        "        # We need to pad the Noise Q/K to match the dimension of Signal Q/K for cleaner code,\n",
        "        # OR we just run separate kernels. Separate kernels is better.\n",
        "\n",
        "        # 5. FlashAttention Calls\n",
        "        # Signal Output: SDPA(Q_sig, K_sig, V_sig)\n",
        "        # Transpose for SDPA: [B, H, T, D]\n",
        "        sig_out = F.scaled_dot_product_attention(\n",
        "            q_sig_full.transpose(1, 2),\n",
        "            k_sig_full.transpose(1, 2),\n",
        "            v_sig.transpose(1, 2),\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # Noise Output: SDPA(Q_noise, K_noise, V_sig)\n",
        "        # Note: We use V_sig for both. This means the noise head decides \"which parts of Value\" are noise.\n",
        "        noise_out = F.scaled_dot_product_attention(\n",
        "            q_noise.transpose(1, 2),\n",
        "            k_noise.transpose(1, 2),\n",
        "            v_sig.transpose(1, 2),\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # 6. Differential Subtraction (AV - BV)\n",
        "        # Calculate Lambda\n",
        "        lam = torch.exp(torch.sum(self.lambda_q * self.lambda_k, dim=-1)) + self.lambda_init\n",
        "        lam = lam.view(1, self.nh, 1, 1) # [1, H, 1, 1]\n",
        "\n",
        "        diff_out = sig_out - lam * noise_out\n",
        "\n",
        "        # 7. Norm & Proj\n",
        "        # [B, H, T, D] -> [B, T, H, D]\n",
        "        diff_out = diff_out.transpose(1, 2)\n",
        "        diff_out = self.head_norm(diff_out)\n",
        "        diff_out = diff_out * (1 - self.lambda_init)\n",
        "\n",
        "        return self.c_proj(diff_out.flatten(2))\n",
        "\n",
        "# Baseline MLA for Comparison\n",
        "class Baseline_MLA(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "        self.W_UQ = nn.Linear(self.q_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False)\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "        self.W_UK = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UV = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "        cQ = self.W_DQ(x)\n",
        "        q = self.W_UQ(cQ).view(B, T, self.nh, self.hd)\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "        cKV = self.W_DKV(x)\n",
        "        k = self.W_UK(cKV).view(B, T, self.nh, self.hd)\n",
        "        v = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "        q_rope, k_rope = apply_rotary_emb(q_rope, k_rope, freqs_cis)\n",
        "        q_full = torch.cat([q, q_rope], dim=-1)\n",
        "        k_full = torch.cat([k, k_rope], dim=-1)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q_full.transpose(1, 2), k_full.transpose(1, 2), v.transpose(1, 2), is_causal=True\n",
        "        )\n",
        "        return self.c_proj(y.transpose(1, 2).flatten(2))\n",
        "\n",
        "# 6. Transformer Backbone\n",
        "# ------------------------------------------------------------------------------\n",
        "class LargeTransformer(nn.Module):\n",
        "    def __init__(self, config: ModelConfig, model_type=\"mla\"):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.dim)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(config.n_layers):\n",
        "            self.layers.append(nn.ModuleDict({\n",
        "                'norm1': RMSNorm(config.dim),\n",
        "                'attn': Optimized_CMD_MLA(config) if model_type == \"cmd_mla\" else Baseline_MLA(config),\n",
        "                'norm2': RMSNorm(config.dim),\n",
        "                'mlp': SwiGLU(config.dim, 4 * config.dim)\n",
        "            }))\n",
        "        self.final_norm = RMSNorm(config.dim)\n",
        "        self.lm_head = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_emb.weight\n",
        "        self.freqs_cis = precompute_freqs_cis(config.rope_head_dim, config.max_seq_len * 2)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        x = self.token_emb(idx)\n",
        "        freqs = self.freqs_cis[:T]\n",
        "        for layer in self.layers:\n",
        "            x = x + layer['attn'](layer['norm1'](x), freqs)\n",
        "            x = x + layer['mlp'](layer['norm2'](x))\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "# 7. Training Harness (Scaled)\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_scaled_experiment(name, model_type, train_steps=500, batch_size=8):\n",
        "    print(f\"\\n========================================================\")\n",
        "    print(f\"Starting Scaled Run: {name} (A100 Optimized)\")\n",
        "    print(f\"========================================================\")\n",
        "\n",
        "    cfg = ModelConfig()\n",
        "    model = LargeTransformer(cfg, model_type=model_type).to(device)\n",
        "\n",
        "    # Compile model for speed (PyTorch 2.0)\n",
        "    # model = torch.compile(model)\n",
        "\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model Parameters: {params/1e6:.2f}M\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_steps)\n",
        "\n",
        "    # AMP Scaler\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    model.train()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start_time = time.time()\n",
        "    losses = []\n",
        "\n",
        "    # Data Iterator\n",
        "    def get_batch():\n",
        "        ix = torch.randint(len(train_data) - cfg.max_seq_len, (batch_size,))\n",
        "        x = torch.stack([train_data[i:i+cfg.max_seq_len] for i in ix])\n",
        "        y = torch.stack([train_data[i+1:i+cfg.max_seq_len+1] for i in ix])\n",
        "        return x.to(device), y.to(device)\n",
        "\n",
        "    for step in range(train_steps):\n",
        "        x, y = get_batch()\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            curr_loss = sum(losses[-10:]) / (len(losses[-10:]) + 1e-9)\n",
        "            print(f\"Step {step:04d} | Loss: {curr_loss:.4f} | Tokens: {step*batch_size*cfg.max_seq_len:,}\")\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    total_time = time.time() - start_time\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1024**3 # GB\n",
        "    final_loss = sum(losses[-50:]) / 50\n",
        "    throughput = (train_steps * batch_size * cfg.max_seq_len) / total_time\n",
        "\n",
        "    print(f\"--- Results: {name} ---\")\n",
        "    print(f\"Final Loss: {final_loss:.4f}\")\n",
        "    print(f\"Peak VRAM:  {peak_mem:.2f} GB\")\n",
        "    print(f\"Throughput: {throughput:.0f} tokens/sec\")\n",
        "\n",
        "    del model, optimizer, scaler\n",
        "    torch.cuda.empty_cache()\n",
        "    return final_loss, peak_mem, throughput\n",
        "\n",
        "# 8. Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "# We run shorter steps to fit 1 hour limit, but batch size * seq_len is large\n",
        "# 2048 seq len * 8 batch = 16k tokens per step.\n",
        "# 1000 steps = 16M tokens trained.\n",
        "STEPS = 1000\n",
        "\n",
        "print(\"Warming up CUDA...\")\n",
        "loss_base, mem_base, speed_base = run_scaled_experiment(\"Baseline MLA\", \"mla\", train_steps=STEPS)\n",
        "loss_cmd, mem_cmd, speed_cmd = run_scaled_experiment(\"CMD-MLA (Ours)\", \"cmd_mla\", train_steps=STEPS)\n",
        "\n",
        "print(\"\\n\\n========================================================\")\n",
        "print(f\"FINAL HEAD-TO-HEAD REPORT (WikiText-103, ~200M Params)\")\n",
        "print(\"========================================================\")\n",
        "print(f\"{'Metric':<15} | {'MLA (Base)':<15} | {'CMD-MLA':<15} | {'Delta'}\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(f\"{'Final Loss':<15} | {loss_base:.4f}          | {loss_cmd:.4f}          | {loss_cmd - loss_base:+.4f}\")\n",
        "print(f\"{'VRAM (GB)':<15} | {mem_base:.2f} GB          | {mem_cmd:.2f} GB          | {mem_cmd - mem_base:+.2f} GB\")\n",
        "print(f\"{'Speed':<15} | {speed_base:.0f} tok/s     | {speed_cmd:.0f} tok/s     | {(speed_cmd-speed_base)/speed_base*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "df88dea6285e4d4d805910ca6f30528f",
            "dc971cbfc3ae4b6a890054837778db47",
            "caaa156ba2b14a029847d22671137cfd",
            "fa534c014f10463c8fe6d04650a0e5f2",
            "5554338a13d147cdadbeca84015f040f",
            "48f42621fc1043cc88127b666d40328e",
            "cf5ca29975844357a442d5d6aa3a8aca",
            "2778e05cab2f48c4890b536d38b90b0c",
            "9a4ca7100642431aa4a9761444d54872",
            "e39fcf042dea4d31b30c793fbbb0715a",
            "5fa964950f5e43c49eb1505926d124ac",
            "74fab9c4805348879e6bae11ce6d0d7c",
            "bb1996d20ed847e193eb66d045c86a86",
            "e92cce408d1b4d989969a24bcf826d74",
            "addc248892c44b05854fa51952fafd0e",
            "9bb6d1a41b3743499e3ed887f0914463",
            "e93a98977ca74e908bd0b1f291bffdad",
            "2d4f3e4bca2c49d3a52c21b41e5a60a6",
            "752cde32cf2640f2ac37cca303b54958",
            "c0c8d216abd24a22808f99fe17dc7f3c",
            "3c0ef1643e89413a99c640ac4d2f5707",
            "ef686a670ccb4b7ba6298de28438b353",
            "386982e4e51d43c58d083cd2c8f55b0a",
            "9518c606e7fd41749ce6596e59588dba",
            "42cf86921b01406f9e86883e92dc3445",
            "2d569008039f47a6b2ffeb83d35fa5ed",
            "948aa484093e49b6b229fa47e22980a3",
            "9a93a066e2d34af283e767c199476a56",
            "73432eb2fd5a48f398b062cde6cbf58d",
            "1a71295b29254eeea0b550a0d088fde6",
            "cd4ee7c795334b119a28f3dff1ca8d0c",
            "142cf8d544bc468685646ccbc50dd894",
            "698b992faf9c4861b8fd8b4b00dabcd7",
            "e947d41b1e794d48ba9dfde8ea8fa9c0",
            "c54d6bd389ae4b2ba8dca6a2494683b7",
            "5fd8fdef122f4c76b5f490a4d698305e",
            "be565eb75b1141efbfc3e92cbdb6879f",
            "547597b0f3194fefa36dda5c0c7f4d61",
            "a08ec850d0574eac839ad77a09a98337",
            "7e964d8fb30947c1b5bb02639826f2be",
            "c1a5dd957bb7404f982632454e8798e7",
            "e47c44fe38294729b2bca49679017bde",
            "ee106c34bc4e40d5825e4af6cec4af02",
            "b883507fd9dc429494d63358709388c1",
            "cb92b160850f4115bf06a7e16fd2fda2",
            "12212d2436fe4e71b4fa28ac93a3bdcf",
            "628ee12b1282465cbc4b38ec9003a12a",
            "40caa092d377480baa14641fa67ca0be",
            "4aa4e59857254846ae53c2d783871fb5",
            "a07e1275f04e492e9e2d8e1b9f3b619f",
            "b4b543b4ba7d4e12a9cef23026d22362",
            "290bc9874a1a47fe821ca787331d09f3",
            "dae02565e9a44ed1a113163d9c67440d",
            "00723a371f8e4c569f301e57d6cd79c7",
            "5166237b62be443b81816137f85184c7",
            "724586c79dd447b68cb5ada52fe31492",
            "97fe260de3a241099de6e28d1c10017a",
            "d302e80c9b0a4f9392757bd1d8e153c1",
            "53078b1b75ba4f4e862859d500302b00",
            "82a310255b98435089379e512c1dfa8f",
            "722b3986786f473ab471b7527bf58f9e",
            "c489f2d25d1442a385549db62e42d80a",
            "f8ee7832d1be41519dc673418574c780",
            "374fefdb25e04f43a80f6b8892b374d8",
            "b8f42485313b4800bddc64a4db3e0cb0",
            "7cd9a18bd17f47a1afd022fbcf948625",
            "a8572a57ddea477e8869761f2bd198f4",
            "7d21444816f64ccda62aa805f8c9ebab",
            "d4bab62ddfb64804a48d89fb6625cc1d",
            "c47a0d5d29c74d599914014a00e339bc",
            "c21a11051cd849988dd71dbee91f748b",
            "23e2c693e3114884a9c25db811b8f891",
            "bc83b8e2f3614890af91f4d93969bae6",
            "c3b2aa6a57684f9db2da477679784e55",
            "de0fbbe94da54a6f951c83b339ae5fda",
            "84f20124bcb2419c8e9faec8b2f39bdb",
            "0234eea9208a49879f29974d00fbde38",
            "47ec08b049e144129cc0cf9aed1c723a",
            "b1d9872c47a8457c9c28ffc16de95564",
            "b08cb62f6610441780236e7d905ed0de",
            "a689e8fd67ea4ccf861fd02cc3ba4bab",
            "a19ca5ad7cd74d5a8423c1f4b9cd8dd4",
            "30138db5aa494658aa95cd9f46bf05d7",
            "6f227cccbd50478b82c6a3c232d6ee8b",
            "47d740e4b3714248a9a381fe91274e91",
            "a4e2fc2c08e44717a78ad302a1c0c80b",
            "9a8fe82d61fc4f62899e9cb3c9c731f2",
            "5962d80478074996a006f60ca78791cc"
          ]
        },
        "id": "c-GW8PVwFzo1",
        "outputId": "3ca14e7c-7c12-4247-83d9-ac0a2c0ca8e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute Device: cuda (NVIDIA A100-SXM4-80GB)\n",
            "Loading WikiText-103...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df88dea6285e4d4d805910ca6f30528f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-103-raw-v1/test-00000-of-00001.(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74fab9c4805348879e6bae11ce6d0d7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-103-raw-v1/train-00000-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "386982e4e51d43c58d083cd2c8f55b0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e947d41b1e794d48ba9dfde8ea8fa9c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb92b160850f4115bf06a7e16fd2fda2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "724586c79dd447b68cb5ada52fe31492"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8572a57ddea477e8869761f2bd198f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47ec08b049e144129cc0cf9aed1c723a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing and Flattening in streams...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1802/1802 [01:08<00:00, 26.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting to Tensor...\n",
            "Dataset Loaded: 119,721,490 tokens. Vocab Size: 50257\n",
            "Warming up CUDA...\n",
            "\n",
            "========================================================\n",
            "Starting Scaled Run: Baseline MLA (A100 Optimized)\n",
            "========================================================\n",
            "Model Parameters: 272.48M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1651135275.py:331: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-1651135275.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0000 | Loss: 947.1138 | Tokens: 0\n",
            "Step 0050 | Loss: 51.2598 | Tokens: 819,200\n",
            "Step 0100 | Loss: 29.2583 | Tokens: 1,638,400\n",
            "Step 0150 | Loss: 18.4819 | Tokens: 2,457,600\n",
            "Step 0200 | Loss: 13.0218 | Tokens: 3,276,800\n",
            "Step 0250 | Loss: 10.8060 | Tokens: 4,096,000\n",
            "Step 0300 | Loss: 10.0314 | Tokens: 4,915,200\n",
            "Step 0350 | Loss: 9.1086 | Tokens: 5,734,400\n",
            "Step 0400 | Loss: 8.3287 | Tokens: 6,553,600\n",
            "Step 0450 | Loss: 7.8629 | Tokens: 7,372,800\n",
            "Step 0500 | Loss: 7.7676 | Tokens: 8,192,000\n",
            "Step 0550 | Loss: 7.5067 | Tokens: 9,011,200\n",
            "Step 0600 | Loss: 7.3287 | Tokens: 9,830,400\n",
            "Step 0650 | Loss: 7.1310 | Tokens: 10,649,600\n",
            "Step 0700 | Loss: 6.9860 | Tokens: 11,468,800\n",
            "Step 0750 | Loss: 6.9921 | Tokens: 12,288,000\n",
            "Step 0800 | Loss: 6.9082 | Tokens: 13,107,200\n",
            "Step 0850 | Loss: 6.8473 | Tokens: 13,926,400\n",
            "Step 0900 | Loss: 6.8074 | Tokens: 14,745,600\n",
            "Step 0950 | Loss: 6.8375 | Tokens: 15,564,800\n",
            "--- Results: Baseline MLA ---\n",
            "Final Loss: 6.8146\n",
            "Peak VRAM:  26.63 GB\n",
            "Throughput: 45526 tokens/sec\n",
            "\n",
            "========================================================\n",
            "Starting Scaled Run: CMD-MLA (Ours) (A100 Optimized)\n",
            "========================================================\n",
            "Model Parameters: 273.68M\n",
            "Step 0000 | Loss: 889.7954 | Tokens: 0\n",
            "Step 0050 | Loss: 49.5661 | Tokens: 819,200\n",
            "Step 0100 | Loss: 27.9901 | Tokens: 1,638,400\n",
            "Step 0150 | Loss: 17.5266 | Tokens: 2,457,600\n",
            "Step 0200 | Loss: 12.9907 | Tokens: 3,276,800\n",
            "Step 0250 | Loss: 11.2823 | Tokens: 4,096,000\n",
            "Step 0300 | Loss: 9.3276 | Tokens: 4,915,200\n",
            "Step 0350 | Loss: 8.7991 | Tokens: 5,734,400\n",
            "Step 0400 | Loss: 8.1101 | Tokens: 6,553,600\n",
            "Step 0450 | Loss: 7.6376 | Tokens: 7,372,800\n",
            "Step 0500 | Loss: 7.2575 | Tokens: 8,192,000\n",
            "Step 0550 | Loss: 7.3211 | Tokens: 9,011,200\n",
            "Step 0600 | Loss: 7.0966 | Tokens: 9,830,400\n",
            "Step 0650 | Loss: 7.0859 | Tokens: 10,649,600\n",
            "Step 0700 | Loss: 6.8301 | Tokens: 11,468,800\n",
            "Step 0750 | Loss: 6.7390 | Tokens: 12,288,000\n",
            "Step 0800 | Loss: 6.7708 | Tokens: 13,107,200\n",
            "Step 0850 | Loss: 6.6997 | Tokens: 13,926,400\n",
            "Step 0900 | Loss: 6.6903 | Tokens: 14,745,600\n",
            "Step 0950 | Loss: 6.7286 | Tokens: 15,564,800\n",
            "--- Results: CMD-MLA (Ours) ---\n",
            "Final Loss: 6.6639\n",
            "Peak VRAM:  28.97 GB\n",
            "Throughput: 39297 tokens/sec\n",
            "\n",
            "\n",
            "========================================================\n",
            "FINAL HEAD-TO-HEAD REPORT (WikiText-103, ~200M Params)\n",
            "========================================================\n",
            "Metric          | MLA (Base)      | CMD-MLA         | Delta\n",
            "--------------------------------------------------------\n",
            "Final Loss      | 6.8146          | 6.6639          | -0.1508\n",
            "VRAM (GB)       | 26.63 GB          | 28.97 GB          | +2.34 GB\n",
            "Speed           | 45526 tok/s     | 39297 tok/s     | -13.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class CMDMLAConfig:\n",
        "    dim: int = 1024           # Hidden dimension\n",
        "    n_heads: int = 16         # Number of Signal Heads\n",
        "    max_seq_len: int = 2048\n",
        "\n",
        "    # Latent Compression Dimensions\n",
        "    kv_rank: int = 512        # Size of the compressed KV latent vector\n",
        "    q_rank: int = 1024        # Size of the compressed Query latent vector\n",
        "\n",
        "    # RoPE\n",
        "    rope_head_dim: int = 64   # Dimension used for Rotary Embeddings\n",
        "\n",
        "    # Differential Setup\n",
        "    diff_lambda_init: float = 0.8  # Initial noise subtraction strength\n",
        "\n",
        "class CMD_MLA(nn.Module):\n",
        "    \"\"\"\n",
        "    Common-Mode Differential Multi-Head Latent Attention (CMD-MLA).\n",
        "\n",
        "    A hybrid architecture that uses Low-Rank Compression (MLA) for memory efficiency\n",
        "    and a 1-to-N Differential Noise Head for signal fidelity.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: CMDMLAConfig):\n",
        "        super().__init__()\n",
        "        self.nh = cfg.n_heads\n",
        "        self.hd = cfg.dim // cfg.n_heads\n",
        "        self.kv_rank = cfg.kv_rank\n",
        "        self.q_rank = cfg.q_rank\n",
        "        self.rhd = cfg.rope_head_dim\n",
        "\n",
        "        # 1. Compression (Down-Projections)\n",
        "        self.W_DQ = nn.Linear(cfg.dim, self.q_rank, bias=False)\n",
        "        self.W_DKV = nn.Linear(cfg.dim, self.kv_rank, bias=False)\n",
        "\n",
        "        # 2. Signal Generation (Up-Projections)\n",
        "        # Projects Latent Q/KV -> N Heads\n",
        "        self.W_UQ = nn.Linear(self.q_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UK = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "        self.W_UV = nn.Linear(self.kv_rank, self.nh * self.hd, bias=False)\n",
        "\n",
        "        # 3. Noise Generation (Single Common-Mode Head)\n",
        "        # Projects Latent Q/KV -> 1 Head\n",
        "        self.W_UQ_Noise = nn.Linear(self.q_rank, self.hd, bias=False)\n",
        "        self.W_UK_Noise = nn.Linear(self.kv_rank, self.hd, bias=False)\n",
        "\n",
        "        # 4. RoPE Side-Channel (Decoupled Strategy)\n",
        "        # We apply RoPE only to the Signal Heads to preserve position awareness there.\n",
        "        self.W_QR = nn.Linear(self.q_rank, self.nh * self.rhd, bias=False)\n",
        "        self.W_KR = nn.Linear(cfg.dim, self.rhd, bias=False)\n",
        "\n",
        "        # 5. Differential Gating (Lambda)\n",
        "        # Learnable parameter per head: \"How much noise should this head ignore?\"\n",
        "        # Initialized to allow gradient flow immediately.\n",
        "        self.lambda_init = cfg.diff_lambda_init\n",
        "        self.lambda_q = nn.Parameter(torch.zeros(self.nh, self.hd))\n",
        "        self.lambda_k = nn.Parameter(torch.zeros(self.nh, self.hd))\n",
        "\n",
        "        # 6. Normalization & Output\n",
        "        # Headwise Norm is critical for Differential Attention stability\n",
        "        self.head_norm = RMSNorm(self.hd)\n",
        "        self.c_proj = nn.Linear(cfg.dim, cfg.dim, bias=False)\n",
        "\n",
        "    def _apply_rope(self, q, k, freqs_cis):\n",
        "        # q, k: [B, T, H, D]\n",
        "        q_ = torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2))\n",
        "        k_ = torch.view_as_complex(k.float().reshape(*k.shape[:-1], -1, 2))\n",
        "        freqs = freqs_cis[:q.shape[1]].view(1, q.shape[1], 1, -1)\n",
        "        q_out = torch.view_as_real(q_ * freqs).flatten(3)\n",
        "        k_out = torch.view_as_real(k_ * freqs).flatten(3)\n",
        "        return q_out.type_as(q), k_out.type_as(k)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [Batch, SeqLen, Dim]\n",
        "            freqs_cis: Precomputed RoPE frequencies [SeqLen, Dim/2]\n",
        "        \"\"\"\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # A. Latent Projection\n",
        "        cQ = self.W_DQ(x)   # [B, T, q_rank]\n",
        "        cKV = self.W_DKV(x) # [B, T, kv_rank]\n",
        "\n",
        "        # B. Signal Head Generation\n",
        "        q_sig = self.W_UQ(cQ).view(B, T, self.nh, self.hd)\n",
        "        k_sig = self.W_UK(cKV).view(B, T, self.nh, self.hd)\n",
        "        v_sig = self.W_UV(cKV).view(B, T, self.nh, self.hd)\n",
        "\n",
        "        # C. Common-Mode Noise Generation\n",
        "        # We generate 1 head, then expand to N heads to allow parallel FlashAttention\n",
        "        # This is VRAM efficient because we don't materialize the N x N grid, just the Q/K vectors.\n",
        "        q_noise = self.W_UQ_Noise(cQ).view(B, T, 1, self.hd).expand(B, T, self.nh, self.hd)\n",
        "        k_noise = self.W_UK_Noise(cKV).view(B, T, 1, self.hd).expand(B, T, self.nh, self.hd)\n",
        "\n",
        "        # D. RoPE Injection (Signal Only)\n",
        "        q_rope = self.W_QR(cQ).view(B, T, self.nh, self.rhd)\n",
        "        k_rope = self.W_KR(x).view(B, T, 1, self.rhd).expand(B, T, self.nh, self.rhd)\n",
        "        q_rope, k_rope = self._apply_rope(q_rope, k_rope, freqs_cis)\n",
        "\n",
        "        # Concat content + position for Signal\n",
        "        q_sig_full = torch.cat([q_sig, q_rope], dim=-1)\n",
        "        k_sig_full = torch.cat([k_sig, k_rope], dim=-1)\n",
        "\n",
        "        # E. FlashAttention Decomposition: (A - lambda*B)V = AV - lambda*BV\n",
        "\n",
        "        # 1. Signal Context (AV)\n",
        "        # Transpose to [B, H, T, D] for Torch SDPA\n",
        "        ctx_sig = F.scaled_dot_product_attention(\n",
        "            q_sig_full.transpose(1, 2),\n",
        "            k_sig_full.transpose(1, 2),\n",
        "            v_sig.transpose(1, 2),\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # 2. Noise Context (BV)\n",
        "        # Note: Noise heads use the SAME Value vectors (v_sig) but different Q/K attention patterns\n",
        "        ctx_noise = F.scaled_dot_product_attention(\n",
        "            q_noise.transpose(1, 2),\n",
        "            k_noise.transpose(1, 2),\n",
        "            v_sig.transpose(1, 2),\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        # F. Differential Subtraction\n",
        "        # Calculate Lambda: \\exp(\\lambda_q \\cdot \\lambda_k) + init\n",
        "        lam = torch.exp(torch.sum(self.lambda_q * self.lambda_k, dim=-1)) + self.lambda_init\n",
        "        lam = lam.view(1, self.nh, 1, 1) # Broadcast\n",
        "\n",
        "        # Subtract Common-Mode Noise\n",
        "        ctx_diff = ctx_sig - (lam * ctx_noise)\n",
        "\n",
        "        # G. Normalization & Output\n",
        "        # [B, H, T, D] -> [B, T, H, D]\n",
        "        ctx_diff = ctx_diff.transpose(1, 2)\n",
        "        ctx_diff = self.head_norm(ctx_diff)\n",
        "        ctx_diff = ctx_diff * (1 - self.lambda_init) # Gradient scaling\n",
        "\n",
        "        return self.c_proj(ctx_diff.flatten(2))\n",
        "\n",
        "# Helper: RMSNorm\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        var = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        return x * torch.rsqrt(var + self.eps) * self.weight"
      ],
      "metadata": {
        "id": "3IhKJYF9jf1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}